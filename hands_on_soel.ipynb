{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## What are BNNs posterios really like?"
      ],
      "metadata": {
        "id": "Lw4-lXHDrJdf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "import copy\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Using device:', device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DCUZ9hJWrNKk",
        "outputId": "db895b5d-bce1-4d83-b9cd-a9d3bf25ac9b"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chargement des données\n",
        "\n",
        "Mettre fashion-mnist_train et test.csv dans data/"
      ],
      "metadata": {
        "id": "khPytIeGrSL8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FashionMNIST_CSV(Dataset): # Dataset pytorch\n",
        "    def __init__(self, csv_path, transform=None):\n",
        "        self.data = pd.read_csv(csv_path).values  # Charger le CSV en NumPy\n",
        "        self.labels = self.data[:, 0]  # Première colonne = labels\n",
        "        self.images = self.data[:, 1:].reshape(-1, 1, 28, 28).astype(np.float32)  # Reshape en (N, 1, 28, 28)\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = self.images[idx]\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return torch.tensor(image), torch.tensor(label, dtype=torch.long)\n",
        "\n",
        "def get_fashion_mnist_loaders_from_csv(batch_size=128):\n",
        "    #train-set 60000 images, test-set 10000\n",
        "    train_dataset = FashionMNIST_CSV(\"data/fashion-mnist_train.csv\", transform=lambda x: (x / 255.0))  # Normalisation [0,1]\n",
        "    test_dataset = FashionMNIST_CSV(\"data/fashion-mnist_test.csv\", transform=lambda x: (x / 255.0))\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    return train_loader, test_loader\n"
      ],
      "metadata": {
        "id": "3k2iY_pprUGj"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "next(iter(train_loader))[0].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3OctCwkTsL-L",
        "outputId": "6f15543c-6de3-421c-b992-22d4434000dd"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([64, 1, 28, 28])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bayesian Neural Network (ResNet quoi)"
      ],
      "metadata": {
        "id": "dKHi78y6rcfL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BasicBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, stride=1, use_bias=True, activation=nn.ReLU):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=use_bias)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.activation = activation()\n",
        "\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=use_bias)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_channels != out_channels:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=use_bias),\n",
        "                nn.BatchNorm2d(out_channels)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.activation(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        return self.activation(out)\n",
        "\n",
        "class ResNet20(nn.Module):\n",
        "    def __init__(self, num_classes=10, width=8, activation=nn.ReLU): # width 16 initialement\n",
        "        super(ResNet20, self).__init__()\n",
        "        self.num_blocks = 3  # ResNet-20 has 3 blocks per stage\n",
        "        self.in_channels = width\n",
        "        self.activation = activation()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(1, width, kernel_size=3, stride=1, padding=1, bias=True) #1 canal\n",
        "        self.bn1 = nn.BatchNorm2d(width)\n",
        "\n",
        "        self.stage1 = self._make_layer(width, self.num_blocks, stride=1)\n",
        "        self.stage2 = self._make_layer(width * 2, self.num_blocks, stride=2)\n",
        "        self.stage3 = self._make_layer(width * 4, self.num_blocks, stride=2)\n",
        "\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(width * 4, num_classes)\n",
        "\n",
        "    def _make_layer(self, out_channels, num_blocks, stride):\n",
        "        layers = []\n",
        "        layers.append(BasicBlock(self.in_channels, out_channels, stride))\n",
        "        self.in_channels = out_channels\n",
        "        for _ in range(1, num_blocks):\n",
        "            layers.append(BasicBlock(out_channels, out_channels))\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.activation(self.bn1(self.conv1(x)))\n",
        "        out = self.stage1(out)\n",
        "        out = self.stage2(out)\n",
        "        out = self.stage3(out)\n",
        "        out = self.avg_pool(out)\n",
        "        out = torch.flatten(out, 1)\n",
        "        out = self.fc(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "VqwPMjvcr1yb"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test de l'architecture en supervisé"
      ],
      "metadata": {
        "id": "cP8mDVw9t1wR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialisation du modèle\n",
        "model = ResNet20(num_classes=10).to(device)\n",
        "\n",
        "# Définition de la fonction de perte et de l'optimiseur\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "def train(model, train_loader, criterion, optimizer, device, epochs=10):\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        running_loss = 0.0\n",
        "        for images, labels in train_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {running_loss/len(train_loader):.4f}\")\n",
        "\n",
        "def evaluate(model, test_loader, device):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f'Accuracy on test set: {accuracy:.2f}%')\n",
        "    return accuracy\n",
        "\n",
        "# Entraînement du modèle\n",
        "train(model, train_loader, criterion, optimizer, device, epochs=10)\n",
        "\n",
        "# Évaluation sur le dataset de test\n",
        "evaluate(model, test_loader, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8KE2f19Dt5vo",
        "outputId": "dddde36c-9779-45bc-85df-324d5b9a8037"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: 0.5129\n",
            "Epoch [2/10], Loss: 0.3211\n",
            "Epoch [3/10], Loss: 0.2782\n",
            "Epoch [4/10], Loss: 0.2480\n",
            "Epoch [5/10], Loss: 0.2301\n",
            "Epoch [6/10], Loss: 0.2138\n",
            "Epoch [7/10], Loss: 0.1992\n",
            "Epoch [8/10], Loss: 0.1898\n",
            "Epoch [9/10], Loss: 0.1769\n",
            "Epoch [10/10], Loss: 0.1676\n",
            "Accuracy on test set: 92.08%\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "92.08"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "92% accuracy en 10 epoches, l'architecture semble bonne"
      ],
      "metadata": {
        "id": "wg7p58Kxvsue"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fonctions utilitaires"
      ],
      "metadata": {
        "id": "zMrYfgT8swsb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def set_weights(model, new_weights):\n",
        "    \"\"\"Remplace les poids d'un modèle PyTorch par ceux issus de HMC\"\"\"\n",
        "    state_dict = model.state_dict()  # Dictionnaire des paramètres du modèle\n",
        "    param_keys = [name_layer for name_layer, _ in list(model.named_parameters())]\n",
        "\n",
        "    #param_keys = list(state_dict.keys()) # liste des paramètres du modèle\n",
        "\n",
        "    #with torch.no_grad():\n",
        "    for i, key in enumerate(param_keys):\n",
        "        state_dict[key] = new_weights[i]\n",
        "    model.load_state_dict(state_dict) # charge les nouveaux paramètres\n",
        "\n",
        "def model_predictions(model, dataloader):\n",
        "    \"\"\"Fait des prédictions d'un modèle donné sur tout le dataset et stocke les probabilités\"\"\"\n",
        "\n",
        "    probabilities = []\n",
        "    model.eval()\n",
        "    with torch.no_grad():  # Désactive le calcul des gradients pour la prédiction\n",
        "        for step, (image, label) in enumerate(dataloader):  # itère sur le dataset\n",
        "            image = image.to(device)\n",
        "            logits = model(image)\n",
        "            probs = F.softmax(logits, dim=1)\n",
        "            probabilities.append(probs)\n",
        "    return torch.cat(probabilities, dim=0)  # (dataset_size, num_classes)\n",
        "\n",
        "def BMA_predictions(probabilities):\n",
        "    \"\"\"Fait une prédiction moyenne Bayesian Model Average p(y|x, D) = 1/M * sum_i( p(y|x, wi))\n",
        "    Args:\n",
        "        probabilities: Tensor  (n_models, dataset_size, num_classes)\"\"\"\n",
        "\n",
        "    n_models = probabilities.size(0)  # Nombre de modèles\n",
        "    # Moyenne des prédictions sur la première dimension (celles des modèles)\n",
        "    average_predictions = probabilities.mean(dim=0)  # (dataset_size, num_classes)\n",
        "\n",
        "    # Prédiction finale : la classe qui a la probabilité la plus élevée\n",
        "    class_predict = average_predictions.argmax(dim=1)  # (dataset_size)\n",
        "    return class_predict\n",
        "\n",
        "\n",
        "def calculate_accuracy(predictions, labels):\n",
        "    \"\"\"Calcule l'accuracy en comparant les prédictions aux labels.\"\"\"\n",
        "    correct_predictions = (predictions == labels).sum().item()  # Nombre de prédictions correctes\n",
        "    total_predictions = labels.size(0)  # Nombre total d'exemples\n",
        "    accuracy = correct_predictions / total_predictions  # Accuracy\n",
        "    return accuracy\n"
      ],
      "metadata": {
        "id": "zoZX_E6dsvjr"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fonctions de densité"
      ],
      "metadata": {
        "id": "Ghedw5gmvAZ_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def posterior_log_density_func(model, data_loader, weight_decay, device):\n",
        "    \"\"\"\n",
        "    Approximation stochastique du posterior: log p(w|D) = log p(D|w) + log p(w) - log p(D)\n",
        "    p(D) est une constante, on ne la calcule pas\n",
        "    \"\"\"\n",
        "    # Échantillonne un mini-batch\n",
        "    data, target = next(iter(data_loader))\n",
        "    data, target = data.to(device), target.to(device)\n",
        "\n",
        "    # Calcule la log-vraisemblance (log p(D | w))\n",
        "    output = model(data)\n",
        "    loss = F.cross_entropy(output, target, reduction=\"sum\")  # NLL\n",
        "    log_p_data = -loss  # On prend le négatif car HMC maximise log-likelihood\n",
        "\n",
        "    # Calcule le log-prior (log p(w))\n",
        "    log_p_w = -0.5 * sum(torch.sum(p**2) for p in model.parameters()) * weight_decay # + constante\n",
        "\n",
        "    # f(w) = log p(D | w) + log p(w)\n",
        "    f_w = log_p_data + log_p_w\n",
        "    return f_w\n",
        "\n",
        "\n",
        "def stochastic_grad_f(model, data_loader, weight_decay, device):\n",
        "    \"\"\"Approximation stochastique du gradient ∇f(w) avec un mini-batch\"\"\"\n",
        "    model.zero_grad()\n",
        "\n",
        "    # Échantillonne un mini-batch\n",
        "    data, target = next(iter(data_loader))\n",
        "    data, target = data.to(device), target.to(device)\n",
        "\n",
        "    # Calcule la log-vraisemblance (log p(D | w))\n",
        "    output = model(data)\n",
        "    loss = F.cross_entropy(output, target, reduction=\"sum\")  # NLL\n",
        "    log_p_data = -loss  # On prend le négatif car HMC maximise log-likelihood\n",
        "\n",
        "    # Calcule le log-prior (log p(w))\n",
        "    log_p_w = -0.5 * sum(torch.sum(p**2) for p in model.parameters()) * weight_decay\n",
        "\n",
        "    # f(w) = log p(D | w) + log p(w)\n",
        "    f_w = log_p_data + log_p_w\n",
        "\n",
        "    # Gradient ∇f(w)\n",
        "    (-f_w).backward()\n",
        "    gradients = [p.grad for p in model.parameters()]\n",
        "\n",
        "    return gradients"
      ],
      "metadata": {
        "id": "g5WHndeDvCdo"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Algorithme MHC minibatch"
      ],
      "metadata": {
        "id": "rg390YOPvFII"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#### CONDITION DE METROPOLIS VERSION GPT\n",
        "\n",
        "\n",
        "def sg_leapfrog(w, m, delta, n_leapfrog, model, data_loader, weight_decay, eta, device):\n",
        "    \"\"\"\n",
        "    Stochastic Gradient Leapfrog intégrant le bruit correctif pour SG-HMC.\n",
        "    \"\"\"\n",
        "    for _ in range(n_leapfrog):\n",
        "        # Étape 1 : Mise à jour du momentum\n",
        "        grad_w = stochastic_grad_f(model, data_loader, weight_decay, device)\n",
        "        for i, p in enumerate(model.parameters()):\n",
        "            noise = torch.normal(mean=0, std=np.sqrt(2 * eta * delta), size=p.shape, device=device)\n",
        "            m[i] += (delta / 2) * grad_w[i] + noise\n",
        "\n",
        "        # Étape 2 : Mise à jour des poids\n",
        "        with torch.no_grad():\n",
        "            for i, p in enumerate(model.parameters()):\n",
        "                p += delta * m[i]\n",
        "\n",
        "        # Étape 3 : Dernière mise à jour du momentum\n",
        "        grad_w = stochastic_grad_f(model, data_loader, weight_decay, device)\n",
        "        for i, p in enumerate(model.parameters()):\n",
        "            m[i] += (delta / 2) * grad_w[i]\n",
        "\n",
        "    return w, [-mi for mi in m]  # Inversion du momentum pour réversibilité\n",
        "\n",
        "def SG_HMC(trajectory_length, n_burnin, model, data_loader, delta, n_samples, weight_decay, eta, device):\n",
        "    \"\"\"\n",
        "    Stochastic Gradient Hamiltonian Monte Carlo (SG-HMC)\n",
        "    \"\"\"\n",
        "    n_leapfrog = int(trajectory_length / delta)\n",
        "    model.to(device)\n",
        "\n",
        "    # Initialisation des poids et momentums\n",
        "    w = [p.clone().detach() for p in model.parameters()]\n",
        "    m = [torch.normal(mean=torch.zeros_like(p), std=torch.ones_like(p)) for p in model.parameters()]\n",
        "\n",
        "    # Burn-in phase\n",
        "    for _ in tqdm(range(n_burnin), desc=\"Burn-in\"):\n",
        "        m = [torch.normal(mean=torch.zeros_like(p), std=torch.ones_like(p)) for p in model.parameters()]\n",
        "        w, m = sg_leapfrog(w, m, delta, n_leapfrog, model, data_loader, weight_decay, eta, device)\n",
        "\n",
        "    # Échantillonnage\n",
        "    w_samples = []\n",
        "    n_acceptations = 0\n",
        "    for _ in tqdm(range(n_samples), desc=\"Sampling\"):\n",
        "        m = [torch.normal(mean=torch.zeros_like(p), std=torch.ones_like(p)) for p in model.parameters()]\n",
        "        w_proposed, m_proposed = sg_leapfrog(w, m, delta, n_leapfrog, model, data_loader, weight_decay, eta, device)\n",
        "\n",
        "        # Calcul du log-ratio d'acceptation\n",
        "        model_copy = copy.deepcopy(model).to(device)\n",
        "        set_weights(model_copy, w_proposed)\n",
        "\n",
        "        log_acceptance_ratio = (\n",
        "            posterior_log_density_func(model_copy, data_loader, weight_decay, device)\n",
        "            - posterior_log_density_func(model, data_loader, weight_decay, device)\n",
        "            + 0.5 * (sum(mi.pow(2).sum() for mi in m) - sum(mi_prop.pow(2).sum() for mi_prop in m_proposed))\n",
        "        )\n",
        "\n",
        "        # Test d'acceptation Metropolis-Hastings\n",
        "        if torch.rand(1).to(device) < torch.exp(log_acceptance_ratio):\n",
        "            w = [p.clone().detach() for p in w_proposed]\n",
        "            n_acceptations +=1\n",
        "\n",
        "        w_samples.append([p.clone().detach() for p in w])\n",
        "    print( \"n_acceptations : \", n_acceptations)\n",
        "    return w_samples\n"
      ],
      "metadata": {
        "id": "tiRps_WgD4hg"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# CONDITION DE METOPOLIS VERSION LE PAPIER\n",
        "\n",
        "def sg_leapfrog(w, m, delta, n_leapfrog, model, data_loader, weight_decay, eta, device):\n",
        "    \"\"\"\n",
        "    Stochastic Gradient Leapfrog intégrant le bruit correctif pour SG-HMC.\n",
        "    \"\"\"\n",
        "    for _ in range(n_leapfrog):\n",
        "        # Étape 1 : Mise à jour du momentum\n",
        "        grad_w = stochastic_grad_f(model, data_loader, weight_decay, device)\n",
        "        for i, p in enumerate(model.parameters()):\n",
        "            noise = torch.normal(mean=0, std=np.sqrt(2 * eta * delta), size=p.shape, device=device)\n",
        "            m[i] += (delta / 2) * grad_w[i] + noise\n",
        "\n",
        "        # Étape 2 : Mise à jour des poids\n",
        "        with torch.no_grad():\n",
        "            for i, p in enumerate(model.parameters()):\n",
        "                p += delta * m[i]\n",
        "\n",
        "        # Étape 3 : Dernière mise à jour du momentum\n",
        "        grad_w = stochastic_grad_f(model, data_loader, weight_decay, device)\n",
        "        for i, p in enumerate(model.parameters()):\n",
        "            m[i] += (delta / 2) * grad_w[i]\n",
        "\n",
        "    return w, [mi for mi in m]  # Inversion du momentum pour réversibilité\n",
        "\n",
        "def SG_HMC(trajectory_length, n_burnin, model, data_loader, delta, n_samples, weight_decay, eta, device):\n",
        "    \"\"\"\n",
        "    Stochastic Gradient Hamiltonian Monte Carlo (SG-HMC)\n",
        "    \"\"\"\n",
        "    n_leapfrog = int(trajectory_length / delta)\n",
        "    model.to(device)\n",
        "\n",
        "    # Initialisation des poids et momentums\n",
        "    w = [p.clone().detach() for p in model.parameters()]\n",
        "    m = [torch.normal(mean=torch.zeros_like(p), std=torch.ones_like(p)) for p in model.parameters()]\n",
        "\n",
        "    # Burn-in phase\n",
        "    for _ in tqdm(range(n_burnin), desc=\"Burn-in\"):\n",
        "        m = [torch.normal(mean=torch.zeros_like(p), std=torch.ones_like(p)) for p in model.parameters()]\n",
        "        w, m = sg_leapfrog(w, m, delta, n_leapfrog, model, data_loader, weight_decay, eta, device)\n",
        "\n",
        "    # Échantillonnage\n",
        "    w_samples = []\n",
        "    n_acceptations = 0\n",
        "    for _ in tqdm(range(n_samples), desc=\"Sampling\"):\n",
        "        m = [torch.normal(mean=torch.zeros_like(p), std=torch.ones_like(p)) for p in model.parameters()]\n",
        "        w_proposed, m_proposed = sg_leapfrog(w, m, delta, n_leapfrog, model, data_loader, weight_decay, eta, device)\n",
        "\n",
        "        # Calcul du log-ratio d'acceptation\n",
        "        model_copy = copy.deepcopy(model).to(device)\n",
        "        set_weights(model_copy, w_proposed)\n",
        "\n",
        "        f_ratio = posterior_log_density_func(model_copy, data_loader, weight_decay, device) / \\\n",
        "                  posterior_log_density_func(model, data_loader, weight_decay, device)\n",
        "\n",
        "        p_accept = torch.min(\n",
        "            torch.tensor(1.0, device=device),\n",
        "            f_ratio * torch.exp(0.5 * (sum(mi.pow(2).sum() for mi in m) - sum(mi_prop.pow(2).sum() for mi_prop in m_proposed)))\n",
        "        )\n",
        "\n",
        "        # Test d'acceptation Metropolis-Hastings\n",
        "        if torch.rand(1).to(device) < p_accept:\n",
        "            w = [p.clone().detach() for p in w_proposed]\n",
        "            print(\"poids acceptés\")\n",
        "            n_acceptations +=1\n",
        "\n",
        "        w_samples.append([p.clone().detach() for p in w])\n",
        "    print( \"n_acceptations : \", n_acceptations)\n",
        "    return w_samples\n"
      ],
      "metadata": {
        "id": "t8QGcYAlOzxp"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Script global"
      ],
      "metadata": {
        "id": "gkU-LzxLvM-g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Hyper-paramètres ###\n",
        "\n",
        "# Paramètres du prior gaussien :\n",
        "prior_variance = 1/5\n",
        "std = np.sqrt(prior_variance)\n",
        "weight_decay = 1/(std**2) # definition\n",
        "\n",
        "# Paramètres HMC :\n",
        "trajectory_length = (np.pi*std)/2 # formule du papier\n",
        "n_burnin = 20 # 50 dans le papier\n",
        "delta = 5e-2 # 1e-5, 5e-5, 1e-4 dans le papier, 1e-3 = 3h,\n",
        "n_samples = 300\n",
        "eta = 1e-6\n",
        "\n",
        "# Initialise le dataloader\n",
        "batch_size = 128*4\n",
        "train_loader, test_loader = get_fashion_mnist_loaders_from_csv(batch_size=batch_size)\n",
        "\n",
        "# Choix du modèle et des fonctions\n",
        "model = ResNet20(num_classes=10).to(device)\n",
        "f = posterior_log_density_func(model, train_loader, weight_decay, device)\n",
        "grad_f = stochastic_grad_f(model, train_loader, weight_decay, device)\n",
        "\n",
        "# Initialise les poids du modèle suivant le prior\n",
        "w_init = [torch.normal(mean=0, std=std, size=p.shape) for p in model.parameters()]\n",
        "set_weights(model, w_init)\n",
        "\n",
        "### HMC et BMA predictions ###\n",
        "w_samples = SG_HMC(\n",
        "    trajectory_length=trajectory_length,\n",
        "    n_burnin=n_burnin,\n",
        "    model=model,\n",
        "    data_loader=train_loader,\n",
        "    delta=delta,\n",
        "    n_samples=n_samples,\n",
        "    weight_decay=weight_decay,\n",
        "    eta=eta,\n",
        "    device=device\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_tlhyMSgvOx_",
        "outputId": "4f39220c-6588-4847-b4f5-bf6eca413148"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Burn-in: 100%|██████████| 20/20 [00:30<00:00,  1.52s/it]\n",
            "Sampling: 100%|██████████| 300/300 [07:29<00:00,  1.50s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "n_acceptations :  0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "print(f\" Taille en mémoire des poids du ResNet : {sys.getsizeof(w_samples) / 1000 :.2f} GB\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hy2XooDk8zFh",
        "outputId": "62a505f5-cd02-482d-b4f7-3583a1e2923c"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Taille en mémoire des poids du ResNet : 2.52 GB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Faire des prédictions avec les échantillons de poids\n",
        "probabilities = torch.zeros(len(w_samples), len(test_loader.dataset), 10, device=device)\n",
        "\n",
        "for i, w_sample in enumerate(tqdm(w_samples, desc=\"Processing samples\")):\n",
        "    set_weights(model, w_sample)\n",
        "    probabilities[i] = model_predictions(model, test_loader)  # Remplissage direct\n",
        "\n",
        "class_predictions = BMA_predictions(probabilities)\n",
        "\n",
        "accuracy = calculate_accuracy(class_predictions, torch.tensor(test_loader.dataset.labels).to(device))\n",
        "print(f'\\nAccuracy: {accuracy * 100:.2f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SroBM9K-4nCN",
        "outputId": "b0e60718-c08b-48f6-91a2-fc1e3329c486"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing samples: 100%|██████████| 300/300 [01:45<00:00,  2.83it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Accuracy: 10.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Expériences du papier à reproduire\n",
        "\n",
        "Idées:\n",
        "- 3 chaînes vs 1 chaine\n",
        "- Différents hyper paramètres à compute fixé\n",
        "- visualisation du postérieur avec 3 points en 2D ! Très stylé"
      ],
      "metadata": {
        "id": "d69i3Nml3KNF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "X-1DbZdN3NbN"
      }
    }
  ]
}