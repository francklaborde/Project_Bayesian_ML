{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lw4-lXHDrJdf"
      },
      "source": [
        "## What are BNNs posterios really like?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DCUZ9hJWrNKk",
        "outputId": "db895b5d-bce1-4d83-b9cd-a9d3bf25ac9b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "import copy\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Using device:', device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "khPytIeGrSL8"
      },
      "source": [
        "## Chargement des données\n",
        "\n",
        "Mettre fashion-mnist_train et test.csv dans data/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "3k2iY_pprUGj"
      },
      "outputs": [],
      "source": [
        "class FashionMNIST_CSV(Dataset): # Dataset pytorch\n",
        "    def __init__(self, csv_path, transform=None):\n",
        "        self.data = pd.read_csv(csv_path).values  # Charger le CSV en NumPy\n",
        "        self.labels = self.data[:, 0]  # Première colonne = labels\n",
        "        self.images = self.data[:, 1:].reshape(-1, 1, 28, 28).astype(np.float32)  # Reshape en (N, 1, 28, 28)\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = self.images[idx]\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return torch.tensor(image), torch.tensor(label, dtype=torch.long)\n",
        "\n",
        "def get_fashion_mnist_loaders_from_csv(batch_size=128):\n",
        "    #train-set 60000 images, test-set 10000\n",
        "    train_dataset = FashionMNIST_CSV(\"data/fashion-mnist_train.csv\", transform=lambda x: (x / 255.0))  # Normalisation [0,1]\n",
        "    test_dataset = FashionMNIST_CSV(\"data/fashion-mnist_test.csv\", transform=lambda x: (x / 255.0))\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    return train_loader, test_loader\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3OctCwkTsL-L",
        "outputId": "6f15543c-6de3-421c-b992-22d4434000dd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([64, 1, 28, 28])"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "next(iter(train_loader))[0].shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dKHi78y6rcfL"
      },
      "source": [
        "## Bayesian Neural Network (ResNet quoi)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "VqwPMjvcr1yb"
      },
      "outputs": [],
      "source": [
        "class BasicBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, stride=1, use_bias=True, activation=nn.ReLU):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=use_bias)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.activation = activation()\n",
        "\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=use_bias)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_channels != out_channels:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=use_bias),\n",
        "                nn.BatchNorm2d(out_channels)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.activation(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        return self.activation(out)\n",
        "\n",
        "class ResNet20(nn.Module):\n",
        "    def __init__(self, num_classes=10, width=8, activation=nn.ReLU): # width 16 initialement\n",
        "        super(ResNet20, self).__init__()\n",
        "        self.num_blocks = 3  # ResNet-20 has 3 blocks per stage\n",
        "        self.in_channels = width\n",
        "        self.activation = activation()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(1, width, kernel_size=3, stride=1, padding=1, bias=True) #1 canal\n",
        "        self.bn1 = nn.BatchNorm2d(width)\n",
        "\n",
        "        self.stage1 = self._make_layer(width, self.num_blocks, stride=1)\n",
        "        self.stage2 = self._make_layer(width * 2, self.num_blocks, stride=2)\n",
        "        self.stage3 = self._make_layer(width * 4, self.num_blocks, stride=2)\n",
        "\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(width * 4, num_classes)\n",
        "\n",
        "    def _make_layer(self, out_channels, num_blocks, stride):\n",
        "        layers = []\n",
        "        layers.append(BasicBlock(self.in_channels, out_channels, stride))\n",
        "        self.in_channels = out_channels\n",
        "        for _ in range(1, num_blocks):\n",
        "            layers.append(BasicBlock(out_channels, out_channels))\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.activation(self.bn1(self.conv1(x)))\n",
        "        out = self.stage1(out)\n",
        "        out = self.stage2(out)\n",
        "        out = self.stage3(out)\n",
        "        out = self.avg_pool(out)\n",
        "        out = torch.flatten(out, 1)\n",
        "        out = self.fc(out)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cP8mDVw9t1wR"
      },
      "source": [
        "## Test de l'architecture en supervisé"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8KE2f19Dt5vo",
        "outputId": "dddde36c-9779-45bc-85df-324d5b9a8037"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/10], Loss: 0.5129\n",
            "Epoch [2/10], Loss: 0.3211\n",
            "Epoch [3/10], Loss: 0.2782\n",
            "Epoch [4/10], Loss: 0.2480\n",
            "Epoch [5/10], Loss: 0.2301\n",
            "Epoch [6/10], Loss: 0.2138\n",
            "Epoch [7/10], Loss: 0.1992\n",
            "Epoch [8/10], Loss: 0.1898\n",
            "Epoch [9/10], Loss: 0.1769\n",
            "Epoch [10/10], Loss: 0.1676\n",
            "Accuracy on test set: 92.08%\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "92.08"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Initialisation du modèle\n",
        "model = ResNet20(num_classes=10).to(device)\n",
        "\n",
        "# Définition de la fonction de perte et de l'optimiseur\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "def train(model, train_loader, criterion, optimizer, device, epochs=10):\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        running_loss = 0.0\n",
        "        for images, labels in train_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {running_loss/len(train_loader):.4f}\")\n",
        "\n",
        "def evaluate(model, test_loader, device):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f'Accuracy on test set: {accuracy:.2f}%')\n",
        "    return accuracy\n",
        "\n",
        "# Entraînement du modèle\n",
        "train(model, train_loader, criterion, optimizer, device, epochs=10)\n",
        "\n",
        "# Évaluation sur le dataset de test\n",
        "evaluate(model, test_loader, device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wg7p58Kxvsue"
      },
      "source": [
        "92% accuracy en 10 epoches, l'architecture semble bonne"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zMrYfgT8swsb"
      },
      "source": [
        "## Fonctions utilitaires"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "zoZX_E6dsvjr"
      },
      "outputs": [],
      "source": [
        "def set_weights(model, new_weights):\n",
        "    \"\"\"Remplace les poids d'un modèle PyTorch par ceux issus de HMC\"\"\"\n",
        "    state_dict = model.state_dict()  # Dictionnaire des paramètres du modèle\n",
        "    param_keys = [name_layer for name_layer, _ in list(model.named_parameters())]\n",
        "\n",
        "    #param_keys = list(state_dict.keys()) # liste des paramètres du modèle\n",
        "\n",
        "    #with torch.no_grad():\n",
        "    for i, key in enumerate(param_keys):\n",
        "        state_dict[key] = new_weights[i]\n",
        "    model.load_state_dict(state_dict) # charge les nouveaux paramètres\n",
        "\n",
        "def model_predictions(model, dataloader):\n",
        "    \"\"\"Fait des prédictions d'un modèle donné sur tout le dataset et stocke les probabilités\"\"\"\n",
        "\n",
        "    probabilities = []\n",
        "    model.eval()\n",
        "    with torch.no_grad():  # Désactive le calcul des gradients pour la prédiction\n",
        "        for step, (image, label) in enumerate(dataloader):  # itère sur le dataset\n",
        "            image = image.to(device)\n",
        "            logits = model(image)\n",
        "            probs = F.softmax(logits, dim=1)\n",
        "            probabilities.append(probs)\n",
        "    return torch.cat(probabilities, dim=0)  # (dataset_size, num_classes)\n",
        "\n",
        "def BMA_predictions(probabilities):\n",
        "    \"\"\"Fait une prédiction moyenne Bayesian Model Average p(y|x, D) = 1/M * sum_i( p(y|x, wi))\n",
        "    Args:\n",
        "        probabilities: Tensor  (n_models, dataset_size, num_classes)\"\"\"\n",
        "\n",
        "    n_models = probabilities.size(0)  # Nombre de modèles\n",
        "    # Moyenne des prédictions sur la première dimension (celles des modèles)\n",
        "    average_predictions = probabilities.mean(dim=0)  # (dataset_size, num_classes)\n",
        "\n",
        "    # Prédiction finale : la classe qui a la probabilité la plus élevée\n",
        "    class_predict = average_predictions.argmax(dim=1)  # (dataset_size)\n",
        "    return class_predict\n",
        "\n",
        "\n",
        "def calculate_accuracy(predictions, labels):\n",
        "    \"\"\"Calcule l'accuracy en comparant les prédictions aux labels.\"\"\"\n",
        "    correct_predictions = (predictions == labels).sum().item()  # Nombre de prédictions correctes\n",
        "    total_predictions = labels.size(0)  # Nombre total d'exemples\n",
        "    accuracy = correct_predictions / total_predictions  # Accuracy\n",
        "    return accuracy\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ghedw5gmvAZ_"
      },
      "source": [
        "Fonctions de densité"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "g5WHndeDvCdo"
      },
      "outputs": [],
      "source": [
        "def posterior_log_density_func(model, data_loader, weight_decay, device, compute_grad=True):\n",
        "    \"\"\"\n",
        "    Approximation stochastique du posterior: log p(w|D) = log p(D|w) + log p(w)\n",
        "    Renvoie f(w) et ∇f(w) si `compute_grad=True`\n",
        "    \"\"\"\n",
        "    model.zero_grad()\n",
        "\n",
        "    # Échantillonne un mini-batch\n",
        "    data, target = next(iter(data_loader))\n",
        "    data, target = data.to(device), target.to(device)\n",
        "\n",
        "    # Log-vraisemblance log p(D | w)\n",
        "    output = model(data)\n",
        "    loss = F.cross_entropy(output, target, reduction=\"sum\")\n",
        "    log_p_data = -loss\n",
        "\n",
        "    # Log-prior log p(w)\n",
        "    log_p_w = -0.5 * sum(torch.sum(p**2) for p in model.parameters()) * weight_decay\n",
        "\n",
        "    # Calcul de f(w)\n",
        "    f_w = log_p_data + log_p_w\n",
        "\n",
        "    # Calcul du gradient si nécessaire\n",
        "    gradients = None\n",
        "    if compute_grad:\n",
        "        f_w.backward()\n",
        "        gradients = [p.grad.clone() for p in model.parameters()]\n",
        "\n",
        "    return f_w, gradients"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rg390YOPvFII"
      },
      "source": [
        "## Algorithme MHC minibatch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "tiRps_WgD4hg"
      },
      "outputs": [],
      "source": [
        "\n",
        "#### CONDITION DE METROPOLIS VERSION GPT\n",
        "\n",
        "\n",
        "def sg_leapfrog(w, m, delta, n_leapfrog, model, data_loader, weight_decay, eta, device):\n",
        "    \"\"\"\n",
        "    Stochastic Gradient Leapfrog intégrant le bruit correctif pour SG-HMC.\n",
        "    \"\"\"\n",
        "    for step in range(n_leapfrog):\n",
        "        # Étape 1 : Mise à jour du momentum\n",
        "        _, grad_w = posterior_log_density_func(model, data_loader, weight_decay, device, compute_grad=True)\n",
        "        #for i, g in enumerate(grad_w):\n",
        "        #    if i < 3:\n",
        "        #        print(f\"Leafrog step {step} Gradient Param {i}: Mean {g.mean().item():.5f}, Std {g.std().item():.5f}\")\n",
        "\n",
        "\n",
        "        for i, p in enumerate(model.parameters()):\n",
        "            noise = torch.normal(mean=0, std=np.sqrt(2 * eta * delta), size=p.shape, device=device)\n",
        "            m[i] += (delta / 2) * grad_w[i] + noise\n",
        "\n",
        "        # Étape 2 : Mise à jour des poids\n",
        "        with torch.no_grad():\n",
        "            for i, p in enumerate(model.parameters()):\n",
        "                p += delta * m[i]\n",
        "\n",
        "        # Étape 3 : Dernière mise à jour du momentum\n",
        "        _, grad_w = posterior_log_density_func(model, data_loader, weight_decay, device, compute_grad=True)\n",
        "        for i, p in enumerate(model.parameters()):\n",
        "            m[i] += (delta / 2) * grad_w[i]\n",
        "        \n",
        "        for i, p in enumerate(model.parameters()):\n",
        "            if torch.isnan(p).any():\n",
        "                print(f\"⚠️ Warning: NaN detected in parameter {i} \")\n",
        "\n",
        "\n",
        "    return w, [-mi for mi in m]  # Inversion du momentum pour réversibilité\n",
        "\n",
        "def SG_HMC(trajectory_length, n_burnin, model, data_loader, delta, n_samples, weight_decay, eta, device):\n",
        "    \"\"\"\n",
        "    Stochastic Gradient Hamiltonian Monte Carlo (SG-HMC)\n",
        "    \"\"\"\n",
        "    n_leapfrog = int(trajectory_length / delta)\n",
        "    print(\"n_leapfrog :\", n_leapfrog)\n",
        "    model.to(device)\n",
        "\n",
        "    # Initialisation des poids et momentums\n",
        "    w = [p.clone().detach() for p in model.parameters()]\n",
        "    m = [torch.normal(mean=torch.zeros_like(p), std=torch.ones_like(p)) for p in model.parameters()]\n",
        "\n",
        "    # Burn-in phase\n",
        "    for _ in tqdm(range(n_burnin), desc=\"Burn-in\"):\n",
        "        m = [torch.normal(mean=torch.zeros_like(p), std=torch.ones_like(p)) for p in model.parameters()]\n",
        "        w, m = sg_leapfrog(w, m, delta, n_leapfrog, model, data_loader, weight_decay, eta, device)\n",
        "\n",
        "    # Échantillonnage\n",
        "    w_samples = []\n",
        "    n_acceptations = 0\n",
        "    for _ in tqdm(range(n_samples), desc=\"Sampling\"):\n",
        "        m = [torch.normal(mean=torch.zeros_like(p), std=torch.ones_like(p)) for p in model.parameters()]\n",
        "        w_proposed, m_proposed = sg_leapfrog(w, m, delta, n_leapfrog, model, data_loader, weight_decay, eta, device)\n",
        "\n",
        "        # Calcul du log-ratio d'acceptation\n",
        "        model_copy = copy.deepcopy(model).to(device)\n",
        "        set_weights(model_copy, w_proposed)\n",
        "\n",
        "        log_acceptance_ratio = (\n",
        "            posterior_log_density_func(model_copy, data_loader, weight_decay, device)\n",
        "            - posterior_log_density_func(model, data_loader, weight_decay, device)\n",
        "            + 0.5 * (sum(mi.pow(2).sum() for mi in m) - sum(mi_prop.pow(2).sum() for mi_prop in m_proposed))\n",
        "        )\n",
        "\n",
        "        # Test d'acceptation Metropolis-Hastings\n",
        "        if torch.rand(1).to(device) < torch.exp(log_acceptance_ratio):\n",
        "            w = [p.clone().detach() for p in w_proposed]\n",
        "            n_acceptations +=1\n",
        "\n",
        "        w_samples.append([p.clone().detach() for p in w])\n",
        "    print( \"n_acceptations : \", n_acceptations)\n",
        "    return w_samples\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t8QGcYAlOzxp"
      },
      "outputs": [],
      "source": [
        "\n",
        "# CONDITION DE METOPOLIS VERSION LE PAPIER\n",
        "\n",
        "def sg_leapfrog(w, m, delta, n_leapfrog, model, data_loader, weight_decay, eta, device):\n",
        "    for _ in range(n_leapfrog):\n",
        "        # Étape 1 : Mise à jour du momentum (avec bruit)\n",
        "        f_w, grad_w = posterior_log_density_func(model, data_loader, weight_decay, device, compute_grad=True)\n",
        "        for i, p in enumerate(model.parameters()):\n",
        "            noise = torch.normal(mean=0, std=np.sqrt(2 * eta * delta), size=p.shape, device=device)\n",
        "            m[i] = m[i] + (delta / 2) * grad_w[i] + noise\n",
        "\n",
        "        # Étape 2 : Mise à jour des poids\n",
        "        with torch.no_grad():\n",
        "            for i, p in enumerate(model.parameters()):\n",
        "                p += delta * m[i]\n",
        "\n",
        "        # Étape 3 : Dernière mise à jour du momentum (sans bruit)\n",
        "        f_w, grad_w = posterior_log_density_func(model, data_loader, weight_decay, device, compute_grad=True)\n",
        "        for i, p in enumerate(model.parameters()):\n",
        "            m[i] = m[i] + (delta / 2) * grad_w[i]\n",
        "    \n",
        "    return w, [-mi for mi in m]  # Inversion du momentum pour réversibilité\n",
        "\n",
        "\n",
        "def SG_HMC(trajectory_length, n_burnin, model, data_loader, delta, n_samples, weight_decay, eta, device):\n",
        "    \"\"\"\n",
        "    Stochastic Gradient Hamiltonian Monte Carlo (SG-HMC)\n",
        "    \"\"\"\n",
        "    n_leapfrog = int(trajectory_length / delta)\n",
        "    model.to(device)\n",
        "\n",
        "    # Initialisation des poids et momentums\n",
        "    w = [p.clone().detach() for p in model.parameters()]\n",
        "    m = [torch.normal(mean=torch.zeros_like(p), std=torch.ones_like(p)) for p in model.parameters()]\n",
        "\n",
        "    # Burn-in phase\n",
        "    for _ in tqdm(range(n_burnin), desc=\"Burn-in\"):\n",
        "        m = [torch.normal(mean=torch.zeros_like(p), std=torch.ones_like(p)) for p in model.parameters()]\n",
        "        w, m = sg_leapfrog(w, m, delta, n_leapfrog, model, data_loader, weight_decay, eta, device)\n",
        "\n",
        "    # Échantillonnage\n",
        "    w_samples = []\n",
        "    n_acceptations = 0\n",
        "    for _ in tqdm(range(n_samples), desc=\"Sampling\"):\n",
        "        m = [torch.normal(mean=torch.zeros_like(p), std=torch.ones_like(p)) for p in model.parameters()]\n",
        "        w_proposed, m_proposed = sg_leapfrog(w, m, delta, n_leapfrog, model, data_loader, weight_decay, eta, device)\n",
        "\n",
        "        # Calcul du log-ratio d'acceptation\n",
        "        model_copy = copy.deepcopy(model).to(device)\n",
        "        set_weights(model_copy, w_proposed)\n",
        "\n",
        "        f_ratio = posterior_log_density_func(model_copy, data_loader, weight_decay, device) / \\\n",
        "                  posterior_log_density_func(model, data_loader, weight_decay, device)\n",
        "\n",
        "        p_accept = torch.min(\n",
        "            torch.tensor(1.0, device=device),\n",
        "            f_ratio * torch.exp(0.5 * (sum(mi.pow(2).sum() for mi in m) - sum(mi_prop.pow(2).sum() for mi_prop in m_proposed)))\n",
        "        )\n",
        "\n",
        "        # Test d'acceptation Metropolis-Hastings\n",
        "        if torch.rand(1).to(device) < p_accept:\n",
        "            w = [p.clone().detach() for p in w_proposed]\n",
        "            print(\"poids acceptés\")\n",
        "            n_acceptations +=1\n",
        "\n",
        "        w_samples.append([p.clone().detach() for p in w])\n",
        "    print( \"n_acceptations : \", n_acceptations)\n",
        "    return w_samples\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gkU-LzxLvM-g"
      },
      "source": [
        "## Script global"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_tlhyMSgvOx_",
        "outputId": "4f39220c-6588-4847-b4f5-bf6eca413148"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "n_leapfrog : 14\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Burn-in:  10%|█         | 2/20 [00:39<05:54, 19.72s/it]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[16], line 27\u001b[0m\n\u001b[0;32m     24\u001b[0m set_weights(model, w_init)\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m### HMC et BMA predictions ###\u001b[39;00m\n\u001b[1;32m---> 27\u001b[0m w_samples \u001b[38;5;241m=\u001b[39m \u001b[43mSG_HMC\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrajectory_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrajectory_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_burnin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_burnin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdelta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[43meta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     36\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\n\u001b[0;32m     37\u001b[0m \u001b[43m)\u001b[49m\n",
            "Cell \u001b[1;32mIn[14], line 52\u001b[0m, in \u001b[0;36mSG_HMC\u001b[1;34m(trajectory_length, n_burnin, model, data_loader, delta, n_samples, weight_decay, eta, device)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(n_burnin), desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBurn-in\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     51\u001b[0m     m \u001b[38;5;241m=\u001b[39m [torch\u001b[38;5;241m.\u001b[39mnormal(mean\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mzeros_like(p), std\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mones_like(p)) \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mparameters()]\n\u001b[1;32m---> 52\u001b[0m     w, m \u001b[38;5;241m=\u001b[39m \u001b[43msg_leapfrog\u001b[49m\u001b[43m(\u001b[49m\u001b[43mw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdelta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_leapfrog\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;66;03m# Échantillonnage\u001b[39;00m\n\u001b[0;32m     55\u001b[0m w_samples \u001b[38;5;241m=\u001b[39m []\n",
            "Cell \u001b[1;32mIn[14], line 26\u001b[0m, in \u001b[0;36msg_leapfrog\u001b[1;34m(w, m, delta, n_leapfrog, model, data_loader, weight_decay, eta, device)\u001b[0m\n\u001b[0;32m     23\u001b[0m         p \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m delta \u001b[38;5;241m*\u001b[39m m[i]\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Étape 3 : Dernière mise à jour du momentum\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m _, grad_w \u001b[38;5;241m=\u001b[39m \u001b[43mposterior_log_density_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompute_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, p \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(model\u001b[38;5;241m.\u001b[39mparameters()):\n\u001b[0;32m     28\u001b[0m     m[i] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (delta \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m) \u001b[38;5;241m*\u001b[39m grad_w[i]\n",
            "Cell \u001b[1;32mIn[5], line 13\u001b[0m, in \u001b[0;36mposterior_log_density_func\u001b[1;34m(model, data_loader, weight_decay, device, compute_grad)\u001b[0m\n\u001b[0;32m     10\u001b[0m data, target \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mto(device), target\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Log-vraisemblance log p(D | w)\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mcross_entropy(output, target, reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msum\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     15\u001b[0m log_p_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mloss\n",
            "File \u001b[1;32mc:\\Users\\soelm\\Documents\\04_Code\\MVA\\MVA_venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\soelm\\Documents\\04_Code\\MVA\\MVA_venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "Cell \u001b[1;32mIn[3], line 51\u001b[0m, in \u001b[0;36mResNet20.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m     50\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn1(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1(x)))\n\u001b[1;32m---> 51\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstage1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     52\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstage2(out)\n\u001b[0;32m     53\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstage3(out)\n",
            "File \u001b[1;32mc:\\Users\\soelm\\Documents\\04_Code\\MVA\\MVA_venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\soelm\\Documents\\04_Code\\MVA\\MVA_venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "File \u001b[1;32mc:\\Users\\soelm\\Documents\\04_Code\\MVA\\MVA_venv\\Lib\\site-packages\\torch\\nn\\modules\\container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
            "File \u001b[1;32mc:\\Users\\soelm\\Documents\\04_Code\\MVA\\MVA_venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\soelm\\Documents\\04_Code\\MVA\\MVA_venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "Cell \u001b[1;32mIn[3], line 20\u001b[0m, in \u001b[0;36mBasicBlock.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m     19\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn1(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1(x)))\n\u001b[1;32m---> 20\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn2(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     21\u001b[0m     out \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshortcut(x)\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation(out)\n",
            "File \u001b[1;32mc:\\Users\\soelm\\Documents\\04_Code\\MVA\\MVA_venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\soelm\\Documents\\04_Code\\MVA\\MVA_venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "File \u001b[1;32mc:\\Users\\soelm\\Documents\\04_Code\\MVA\\MVA_venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:554\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    553\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 554\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\soelm\\Documents\\04_Code\\MVA\\MVA_venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:549\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    537\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    538\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(\n\u001b[0;32m    539\u001b[0m         F\u001b[38;5;241m.\u001b[39mpad(\n\u001b[0;32m    540\u001b[0m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    547\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups,\n\u001b[0;32m    548\u001b[0m     )\n\u001b[1;32m--> 549\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    550\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\n\u001b[0;32m    551\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "### Hyper-paramètres ###\n",
        "\n",
        "# Paramètres du prior gaussien :\n",
        "prior_variance = 1/5\n",
        "std = np.sqrt(prior_variance)\n",
        "weight_decay = 1/(std**2) # definition\n",
        "\n",
        "# Paramètres HMC :\n",
        "trajectory_length = (np.pi*std)/2 # formule du papier\n",
        "n_burnin = 20 # 50 dans le papier\n",
        "delta = 5e-2 # 1e-5, 5e-5, 1e-4 dans le papier, 1e-3 = 3h,\n",
        "n_samples = 300\n",
        "eta = 1e-6\n",
        "\n",
        "# Initialise le dataloader\n",
        "batch_size = 128*4\n",
        "train_loader, test_loader = get_fashion_mnist_loaders_from_csv(batch_size=batch_size)\n",
        "\n",
        "# Choix du modèle et des fonctions\n",
        "model = ResNet20(num_classes=10).to(device)\n",
        "\n",
        "# Initialise les poids du modèle suivant le prior\n",
        "w_init = [torch.normal(mean=0, std=std, size=p.shape) for p in model.parameters()]\n",
        "set_weights(model, w_init)\n",
        "\n",
        "### HMC et BMA predictions ###\n",
        "w_samples = SG_HMC(\n",
        "    trajectory_length=trajectory_length,\n",
        "    n_burnin=n_burnin,\n",
        "    model=model,\n",
        "    data_loader=train_loader,\n",
        "    delta=delta,\n",
        "    n_samples=n_samples,\n",
        "    weight_decay=weight_decay,\n",
        "    eta=eta,\n",
        "    device=device\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hy2XooDk8zFh",
        "outputId": "62a505f5-cd02-482d-b4f7-3583a1e2923c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Taille en mémoire des poids du ResNet : 2.52 GB\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "print(f\" Taille en mémoire des poids du ResNet : {sys.getsizeof(w_samples) / 1000 :.2f} GB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SroBM9K-4nCN",
        "outputId": "b0e60718-c08b-48f6-91a2-fc1e3329c486"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing samples: 100%|██████████| 300/300 [01:45<00:00,  2.83it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Accuracy: 10.00%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Faire des prédictions avec les échantillons de poids\n",
        "probabilities = torch.zeros(len(w_samples), len(test_loader.dataset), 10, device=device)\n",
        "\n",
        "for i, w_sample in enumerate(tqdm(w_samples, desc=\"Processing samples\")):\n",
        "    set_weights(model, w_sample)\n",
        "    probabilities[i] = model_predictions(model, test_loader)  # Remplissage direct\n",
        "\n",
        "class_predictions = BMA_predictions(probabilities)\n",
        "\n",
        "accuracy = calculate_accuracy(class_predictions, torch.tensor(test_loader.dataset.labels).to(device))\n",
        "print(f'\\nAccuracy: {accuracy * 100:.2f}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d69i3Nml3KNF"
      },
      "source": [
        "## Expériences du papier à reproduire\n",
        "\n",
        "Idées:\n",
        "- 3 chaînes vs 1 chaine\n",
        "- Différents hyper paramètres à compute fixé\n",
        "- visualisation du postérieur avec 3 points en 2D ! Très stylé"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X-1DbZdN3NbN"
      },
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "MVA_venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
