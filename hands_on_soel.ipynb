{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lw4-lXHDrJdf"
      },
      "source": [
        "## What are BNNs posterios really like?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DCUZ9hJWrNKk",
        "outputId": "db895b5d-bce1-4d83-b9cd-a9d3bf25ac9b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "import copy\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Using device:', device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "khPytIeGrSL8"
      },
      "source": [
        "## Chargement des données\n",
        "\n",
        "Mettre fashion-mnist_train et test.csv dans data/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "3k2iY_pprUGj"
      },
      "outputs": [],
      "source": [
        "class FashionMNIST_CSV(Dataset): # Dataset pytorch\n",
        "    def __init__(self, csv_path, transform=None):\n",
        "        self.data = pd.read_csv(csv_path).values  # Charger le CSV en NumPy\n",
        "        self.labels = self.data[:, 0]  # Première colonne = labels\n",
        "        self.images = self.data[:, 1:].reshape(-1, 1, 28, 28).astype(np.float32)  # Reshape en (N, 1, 28, 28)\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = self.images[idx]\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return torch.tensor(image), torch.tensor(label, dtype=torch.long)\n",
        "\n",
        "def get_fashion_mnist_loaders_from_csv(batch_size=128):\n",
        "    #train-set 60000 images, test-set 10000\n",
        "    train_dataset = FashionMNIST_CSV(\"../data/fashion-mnist_train.csv\", transform=lambda x: (x / 255.0))  # Normalisation [0,1]\n",
        "    test_dataset = FashionMNIST_CSV(\"../data/fashion-mnist_test.csv\", transform=lambda x: (x / 255.0))\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    return train_loader, test_loader\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dKHi78y6rcfL"
      },
      "source": [
        "## Bayesian Neural Network (ResNet quoi)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "VqwPMjvcr1yb"
      },
      "outputs": [],
      "source": [
        "class BasicBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, stride=1, use_bias=True, activation=nn.ReLU):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=use_bias)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.activation = activation()\n",
        "\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=use_bias)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_channels != out_channels:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=use_bias),\n",
        "                nn.BatchNorm2d(out_channels)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.activation(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        return self.activation(out)\n",
        "\n",
        "class ResNet20(nn.Module):\n",
        "    def __init__(self, num_classes=10, width=8, activation=nn.ReLU): # width 16 initialement\n",
        "        super(ResNet20, self).__init__()\n",
        "        self.num_blocks = 3  # ResNet-20 has 3 blocks per stage\n",
        "        self.in_channels = width\n",
        "        self.activation = activation()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(1, width, kernel_size=3, stride=1, padding=1, bias=True) #1 canal\n",
        "        self.bn1 = nn.BatchNorm2d(width)\n",
        "\n",
        "        self.stage1 = self._make_layer(width, self.num_blocks, stride=1)\n",
        "        self.stage2 = self._make_layer(width * 2, self.num_blocks, stride=2)\n",
        "        self.stage3 = self._make_layer(width * 4, self.num_blocks, stride=2)\n",
        "\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(width * 4, num_classes)\n",
        "\n",
        "    def _make_layer(self, out_channels, num_blocks, stride):\n",
        "        layers = []\n",
        "        layers.append(BasicBlock(self.in_channels, out_channels, stride))\n",
        "        self.in_channels = out_channels\n",
        "        for _ in range(1, num_blocks):\n",
        "            layers.append(BasicBlock(out_channels, out_channels))\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.activation(self.bn1(self.conv1(x)))\n",
        "        out = self.stage1(out)\n",
        "        out = self.stage2(out)\n",
        "        out = self.stage3(out)\n",
        "        out = self.avg_pool(out)\n",
        "        out = torch.flatten(out, 1)\n",
        "        out = self.fc(out)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cP8mDVw9t1wR"
      },
      "source": [
        "## Test de l'architecture en supervisé"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8KE2f19Dt5vo",
        "outputId": "dddde36c-9779-45bc-85df-324d5b9a8037"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/10], Loss: 0.8066\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[10], line 45\u001b[0m\n\u001b[0;32m     43\u001b[0m train_loader, test_loader \u001b[38;5;241m=\u001b[39m get_fashion_mnist_loaders_from_csv(batch_size\u001b[38;5;241m=\u001b[39mbatch_size)\n\u001b[0;32m     44\u001b[0m \u001b[38;5;66;03m# Entraînement du modèle\u001b[39;00m\n\u001b[1;32m---> 45\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;66;03m# Évaluation sur le dataset de test\u001b[39;00m\n\u001b[0;32m     48\u001b[0m evaluate(model, test_loader, device)\n",
            "Cell \u001b[1;32mIn[10], line 12\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, train_loader, criterion, optimizer, device, epochs)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[0;32m     11\u001b[0m     running_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m---> 12\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m images, labels \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[0;32m     13\u001b[0m         images, labels \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     15\u001b[0m         optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
            "File \u001b[1;32mc:\\Users\\Franc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    707\u001b[0m ):\n",
            "File \u001b[1;32mc:\\Users\\Franc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
            "File \u001b[1;32mc:\\Users\\Franc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
            "Cell \u001b[1;32mIn[8], line 18\u001b[0m, in \u001b[0;36mFashionMNIST_CSV.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform:\n\u001b[0;32m     16\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(image)\n\u001b[1;32m---> 18\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m, torch\u001b[38;5;241m.\u001b[39mtensor(label, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong)\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Initialisation du modèle\n",
        "model = ResNet20(num_classes=10).to(device)\n",
        "\n",
        "# Définition de la fonction de perte et de l'optimiseur\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "def train(model, train_loader, criterion, optimizer, device, epochs=10):\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        running_loss = 0.0\n",
        "        for images, labels in train_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {running_loss/len(train_loader):.4f}\")\n",
        "\n",
        "def evaluate(model, test_loader, device):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f'Accuracy on test set: {accuracy:.2f}%')\n",
        "    return accuracy\n",
        "\n",
        "# Initialise le dataloader\n",
        "batch_size = 128*4\n",
        "train_loader, test_loader = get_fashion_mnist_loaders_from_csv(batch_size=batch_size)\n",
        "\n",
        "# Entraînement du modèle\n",
        "train(model, train_loader, criterion, optimizer, device, epochs=10)\n",
        "\n",
        "# Évaluation sur le dataset de test\n",
        "evaluate(model, test_loader, device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wg7p58Kxvsue"
      },
      "source": [
        "92% accuracy en 10 epoches, l'architecture semble bonne"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zMrYfgT8swsb"
      },
      "source": [
        "## Fonctions utilitaires"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "zoZX_E6dsvjr"
      },
      "outputs": [],
      "source": [
        "def set_weights(model, new_weights):\n",
        "    \"\"\"Remplace les poids d'un modèle PyTorch par ceux issus de HMC\"\"\"\n",
        "    state_dict = model.state_dict()  # Dictionnaire des paramètres du modèle\n",
        "    param_keys = [name_layer for name_layer, _ in list(model.named_parameters())]\n",
        "\n",
        "    #param_keys = list(state_dict.keys()) # liste des paramètres du modèle\n",
        "\n",
        "    #with torch.no_grad():\n",
        "    for i, key in enumerate(param_keys):\n",
        "        state_dict[key] = new_weights[i]\n",
        "    model.load_state_dict(state_dict) # charge les nouveaux paramètres\n",
        "\n",
        "def model_predictions(model, dataloader):\n",
        "    \"\"\"Fait des prédictions d'un modèle donné sur tout le dataset et stocke les probabilités\"\"\"\n",
        "\n",
        "    probabilities = []\n",
        "    model.eval()\n",
        "    with torch.no_grad():  # Désactive le calcul des gradients pour la prédiction\n",
        "        for step, (image, label) in enumerate(dataloader):  # itère sur le dataset\n",
        "            image = image.to(device)\n",
        "            logits = model(image)\n",
        "            probs = F.softmax(logits, dim=1)\n",
        "            probabilities.append(probs)\n",
        "    return torch.cat(probabilities, dim=0)  # (dataset_size, num_classes)\n",
        "\n",
        "def BMA_predictions(probabilities):\n",
        "    \"\"\"Fait une prédiction moyenne Bayesian Model Average p(y|x, D) = 1/M * sum_i( p(y|x, wi))\n",
        "    Args:\n",
        "        probabilities: Tensor  (n_models, dataset_size, num_classes)\"\"\"\n",
        "\n",
        "    n_models = probabilities.size(0)  # Nombre de modèles\n",
        "    # Moyenne des prédictions sur la première dimension (celles des modèles)\n",
        "    average_predictions = probabilities.mean(dim=0)  # (dataset_size, num_classes)\n",
        "\n",
        "    # Prédiction finale : la classe qui a la probabilité la plus élevée\n",
        "    class_predict = average_predictions.argmax(dim=1)  # (dataset_size)\n",
        "    return class_predict\n",
        "\n",
        "\n",
        "def calculate_accuracy(predictions, labels):\n",
        "    \"\"\"Calcule l'accuracy en comparant les prédictions aux labels.\"\"\"\n",
        "    correct_predictions = (predictions == labels).sum().item()  # Nombre de prédictions correctes\n",
        "    total_predictions = labels.size(0)  # Nombre total d'exemples\n",
        "    accuracy = correct_predictions / total_predictions  # Accuracy\n",
        "    return accuracy\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ghedw5gmvAZ_"
      },
      "source": [
        "Fonctions de densité"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "g5WHndeDvCdo"
      },
      "outputs": [],
      "source": [
        "def posterior_log_density_func(model, data_loader, weight_decay, device):\n",
        "    \"\"\"\n",
        "    Approximation stochastique du posterior: log p(w|D) = log p(D|w) + log p(w) - log p(D)\n",
        "    p(D) est une constante, on ne la calcule pas\n",
        "    \"\"\"\n",
        "    # Échantillonne un mini-batch\n",
        "    data, target = next(iter(data_loader))\n",
        "    data, target = data.to(device), target.to(device)\n",
        "\n",
        "    # Calcule la log-vraisemblance (log p(D | w))\n",
        "    output = model(data)\n",
        "    loss = F.cross_entropy(output, target, reduction=\"sum\")  # NLL\n",
        "    log_p_data = -loss  # On prend le négatif car HMC maximise log-likelihood\n",
        "\n",
        "    # Calcule le log-prior (log p(w))\n",
        "    log_p_w = -0.5 * sum(torch.sum(p**2) for p in model.parameters()) * weight_decay # + constante\n",
        "\n",
        "    # f(w) = log p(D | w) + log p(w)\n",
        "    f_w = log_p_data + log_p_w\n",
        "    return f_w\n",
        "\n",
        "\n",
        "def stochastic_grad_f(model, data_loader, weight_decay, device):\n",
        "    \"\"\"Approximation stochastique du gradient ∇f(w) avec un mini-batch\"\"\"\n",
        "    model.zero_grad()\n",
        "\n",
        "    # Échantillonne un mini-batch\n",
        "    data, target = next(iter(data_loader))\n",
        "    data, target = data.to(device), target.to(device)\n",
        "\n",
        "    # Calcule la log-vraisemblance (log p(D | w))\n",
        "    output = model(data)\n",
        "    loss = F.cross_entropy(output, target, reduction=\"sum\")  # NLL\n",
        "    log_p_data = -loss  # On prend le négatif car HMC maximise log-likelihood\n",
        "\n",
        "    # Calcule le log-prior (log p(w))\n",
        "    log_p_w = -0.5 * sum(torch.sum(p**2) for p in model.parameters()) * weight_decay\n",
        "\n",
        "    # f(w) = log p(D | w) + log p(w)\n",
        "    f_w = log_p_data + log_p_w\n",
        "\n",
        "    # Gradient ∇f(w)\n",
        "    (-f_w).backward()\n",
        "    gradients = [p.grad for p in model.parameters()]\n",
        "\n",
        "    return gradients"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rg390YOPvFII"
      },
      "source": [
        "## Algorithme MHC minibatch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "tiRps_WgD4hg"
      },
      "outputs": [],
      "source": [
        "\n",
        "#### CONDITION DE METROPOLIS VERSION GPT\n",
        "\n",
        "\n",
        "def sg_leapfrog(w, m, delta, n_leapfrog, model, data_loader, weight_decay, eta, device):\n",
        "    \"\"\"\n",
        "    Stochastic Gradient Leapfrog intégrant le bruit correctif pour SG-HMC.\n",
        "    \"\"\"\n",
        "    for _ in range(n_leapfrog):\n",
        "        # Étape 1 : Mise à jour du momentum\n",
        "        grad_w = stochastic_grad_f(model, data_loader, weight_decay, device)\n",
        "        for i, p in enumerate(model.parameters()):\n",
        "            noise = torch.normal(mean=0, std=np.sqrt(2 * eta * delta), size=p.shape, device=device)\n",
        "            m[i] += (delta / 2) * grad_w[i] + noise\n",
        "\n",
        "        # Étape 2 : Mise à jour des poids\n",
        "        with torch.no_grad():\n",
        "            for i, p in enumerate(model.parameters()):\n",
        "                p += delta * m[i]\n",
        "\n",
        "        # Étape 3 : Dernière mise à jour du momentum\n",
        "        grad_w = stochastic_grad_f(model, data_loader, weight_decay, device)\n",
        "        for i, p in enumerate(model.parameters()):\n",
        "            m[i] += (delta / 2) * grad_w[i]\n",
        "\n",
        "    return w, [-mi for mi in m]  # Inversion du momentum pour réversibilité\n",
        "\n",
        "def SG_HMC(trajectory_length, n_burnin, model, data_loader, delta, n_samples, weight_decay, eta, device):\n",
        "    \"\"\"\n",
        "    Stochastic Gradient Hamiltonian Monte Carlo (SG-HMC)\n",
        "    \"\"\"\n",
        "    n_leapfrog = int(trajectory_length / delta)\n",
        "    model.to(device)\n",
        "\n",
        "    # Initialisation des poids et momentums\n",
        "    w = [p.clone().detach() for p in model.parameters()]\n",
        "    m = [torch.normal(mean=torch.zeros_like(p), std=torch.ones_like(p)) for p in model.parameters()]\n",
        "\n",
        "    # Burn-in phase\n",
        "    for _ in tqdm(range(n_burnin), desc=\"Burn-in\"):\n",
        "        m = [torch.normal(mean=torch.zeros_like(p), std=torch.ones_like(p)) for p in model.parameters()]\n",
        "        w, m = sg_leapfrog(w, m, delta, n_leapfrog, model, data_loader, weight_decay, eta, device)\n",
        "\n",
        "    # Échantillonnage\n",
        "    w_samples = []\n",
        "    n_acceptations = 0\n",
        "    for _ in tqdm(range(n_samples), desc=\"Sampling\"):\n",
        "        m = [torch.normal(mean=torch.zeros_like(p), std=torch.ones_like(p)) for p in model.parameters()]\n",
        "        w_proposed, m_proposed = sg_leapfrog(w, m, delta, n_leapfrog, model, data_loader, weight_decay, eta, device)\n",
        "\n",
        "        # Calcul du log-ratio d'acceptation\n",
        "        model_copy = copy.deepcopy(model).to(device)\n",
        "        set_weights(model_copy, w_proposed)\n",
        "\n",
        "        log_acceptance_ratio = (\n",
        "            posterior_log_density_func(model_copy, data_loader, weight_decay, device)\n",
        "            - posterior_log_density_func(model, data_loader, weight_decay, device)\n",
        "            + 0.5 * (sum(mi.pow(2).sum() for mi in m) - sum(mi_prop.pow(2).sum() for mi_prop in m_proposed))\n",
        "        )\n",
        "\n",
        "        # Test d'acceptation Metropolis-Hastings\n",
        "        if torch.rand(1).to(device) < torch.exp(log_acceptance_ratio):\n",
        "            w = [p.clone().detach() for p in w_proposed]\n",
        "            n_acceptations +=1\n",
        "\n",
        "        w_samples.append([p.clone().detach() for p in w])\n",
        "    print( \"n_acceptations : \", n_acceptations)\n",
        "    return w_samples\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "t8QGcYAlOzxp"
      },
      "outputs": [],
      "source": [
        "\n",
        "# CONDITION DE METOPOLIS VERSION LE PAPIER\n",
        "\n",
        "def sg_leapfrog(w, m, delta, n_leapfrog, model, data_loader, weight_decay, eta, device):\n",
        "    \"\"\"\n",
        "    Stochastic Gradient Leapfrog intégrant le bruit correctif pour SG-HMC.\n",
        "    \"\"\"\n",
        "    for _ in range(n_leapfrog):\n",
        "        # Étape 1 : Mise à jour du momentum\n",
        "        grad_w = stochastic_grad_f(model, data_loader, weight_decay, device)\n",
        "        for i, p in enumerate(model.parameters()):\n",
        "            noise = torch.normal(mean=0, std=np.sqrt(2 * eta * delta), size=p.shape, device=device)\n",
        "            m[i] += (delta / 2) * grad_w[i] + noise\n",
        "\n",
        "        # Étape 2 : Mise à jour des poids\n",
        "        with torch.no_grad():\n",
        "            for i, p in enumerate(model.parameters()):\n",
        "                p += delta * m[i]\n",
        "\n",
        "        # Étape 3 : Dernière mise à jour du momentum\n",
        "        grad_w = stochastic_grad_f(model, data_loader, weight_decay, device)\n",
        "        for i, p in enumerate(model.parameters()):\n",
        "            m[i] += (delta / 2) * grad_w[i]\n",
        "\n",
        "    return w, [mi for mi in m]  # Inversion du momentum pour réversibilité\n",
        "\n",
        "def SG_HMC(trajectory_length, n_burnin, model, data_loader, delta, n_samples, weight_decay, eta, device):\n",
        "    \"\"\"\n",
        "    Stochastic Gradient Hamiltonian Monte Carlo (SG-HMC)\n",
        "    \"\"\"\n",
        "    n_leapfrog = int(trajectory_length / delta)\n",
        "    model.to(device)\n",
        "\n",
        "    # Initialisation des poids et momentums\n",
        "    w = [p.clone().detach() for p in model.parameters()]\n",
        "    m = [torch.normal(mean=torch.zeros_like(p), std=torch.ones_like(p)) for p in model.parameters()]\n",
        "\n",
        "    # Burn-in phase\n",
        "    for _ in tqdm(range(n_burnin), desc=\"Burn-in\"):\n",
        "        m = [torch.normal(mean=torch.zeros_like(p), std=torch.ones_like(p)) for p in model.parameters()]\n",
        "        w, m = sg_leapfrog(w, m, delta, n_leapfrog, model, data_loader, weight_decay, eta, device)\n",
        "\n",
        "    # Échantillonnage\n",
        "    w_samples = []\n",
        "    n_acceptations = 0\n",
        "    for _ in tqdm(range(n_samples), desc=\"Sampling\"):\n",
        "        m = [torch.normal(mean=torch.zeros_like(p), std=torch.ones_like(p)) for p in model.parameters()]\n",
        "        w_proposed, m_proposed = sg_leapfrog(w, m, delta, n_leapfrog, model, data_loader, weight_decay, eta, device)\n",
        "\n",
        "        # Calcul du log-ratio d'acceptation\n",
        "        model_copy = copy.deepcopy(model).to(device)\n",
        "        set_weights(model_copy, w_proposed)\n",
        "\n",
        "        f_ratio = posterior_log_density_func(model_copy, data_loader, weight_decay, device) / \\\n",
        "                  posterior_log_density_func(model, data_loader, weight_decay, device)\n",
        "\n",
        "        p_accept = torch.min(\n",
        "            torch.tensor(1.0, device=device),\n",
        "            f_ratio * torch.exp(0.5 * (sum(mi.pow(2).sum() for mi in m) - sum(mi_prop.pow(2).sum() for mi_prop in m_proposed)))\n",
        "        )\n",
        "\n",
        "        # Test d'acceptation Metropolis-Hastings\n",
        "        if torch.rand(1).to(device) < p_accept:\n",
        "            w = [p.clone().detach() for p in w_proposed]\n",
        "            print(\"poids acceptés\")\n",
        "            n_acceptations +=1\n",
        "\n",
        "        w_samples.append([p.clone().detach() for p in w])\n",
        "    print( \"n_acceptations : \", n_acceptations)\n",
        "    return w_samples\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gkU-LzxLvM-g"
      },
      "source": [
        "## Script global"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_tlhyMSgvOx_",
        "outputId": "4f39220c-6588-4847-b4f5-bf6eca413148"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Burn-in: 100%|██████████| 20/20 [00:30<00:00,  1.52s/it]\n",
            "Sampling: 100%|██████████| 300/300 [07:29<00:00,  1.50s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "n_acceptations :  0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "### Hyper-paramètres ###\n",
        "\n",
        "# Paramètres du prior gaussien :\n",
        "prior_variance = 1/5\n",
        "std = np.sqrt(prior_variance)\n",
        "weight_decay = 1/(std**2) # definition\n",
        "\n",
        "# Paramètres HMC :\n",
        "trajectory_length = (np.pi*std)/2 # formule du papier\n",
        "n_burnin = 20 # 50 dans le papier\n",
        "delta = 5e-2 # 1e-5, 5e-5, 1e-4 dans le papier, 1e-3 = 3h,\n",
        "n_samples = 300\n",
        "eta = 1e-6\n",
        "\n",
        "# Initialise le dataloader\n",
        "batch_size = 128*4\n",
        "train_loader, test_loader = get_fashion_mnist_loaders_from_csv(batch_size=batch_size)\n",
        "\n",
        "# Choix du modèle et des fonctions\n",
        "model = ResNet20(num_classes=10).to(device)\n",
        "f = posterior_log_density_func(model, train_loader, weight_decay, device)\n",
        "grad_f = stochastic_grad_f(model, train_loader, weight_decay, device)\n",
        "\n",
        "# Initialise les poids du modèle suivant le prior\n",
        "w_init = [torch.normal(mean=0, std=std, size=p.shape) for p in model.parameters()]\n",
        "set_weights(model, w_init)\n",
        "\n",
        "### HMC et BMA predictions ###\n",
        "w_samples = SG_HMC(\n",
        "    trajectory_length=trajectory_length,\n",
        "    n_burnin=n_burnin,\n",
        "    model=model,\n",
        "    data_loader=train_loader,\n",
        "    delta=delta,\n",
        "    n_samples=n_samples,\n",
        "    weight_decay=weight_decay,\n",
        "    eta=eta,\n",
        "    device=device\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hy2XooDk8zFh",
        "outputId": "62a505f5-cd02-482d-b4f7-3583a1e2923c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Taille en mémoire des poids du ResNet : 2.52 GB\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "print(f\" Taille en mémoire des poids du ResNet : {sys.getsizeof(w_samples) / 1000 :.2f} GB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SroBM9K-4nCN",
        "outputId": "b0e60718-c08b-48f6-91a2-fc1e3329c486"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing samples: 100%|██████████| 300/300 [01:45<00:00,  2.83it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Accuracy: 10.00%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Faire des prédictions avec les échantillons de poids\n",
        "probabilities = torch.zeros(len(w_samples), len(test_loader.dataset), 10, device=device)\n",
        "\n",
        "for i, w_sample in enumerate(tqdm(w_samples, desc=\"Processing samples\")):\n",
        "    set_weights(model, w_sample)\n",
        "    probabilities[i] = model_predictions(model, test_loader)  # Remplissage direct\n",
        "\n",
        "class_predictions = BMA_predictions(probabilities)\n",
        "\n",
        "accuracy = calculate_accuracy(class_predictions, torch.tensor(test_loader.dataset.labels).to(device))\n",
        "print(f'\\nAccuracy: {accuracy * 100:.2f}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d69i3Nml3KNF"
      },
      "source": [
        "## Expériences du papier à reproduire\n",
        "\n",
        "Idées:\n",
        "- 3 chaînes vs 1 chaine\n",
        "- Différents hyper paramètres à compute fixé\n",
        "- visualisation du postérieur avec 3 points en 2D ! Très stylé"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X-1DbZdN3NbN"
      },
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
