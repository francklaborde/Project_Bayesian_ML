{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nimport torchvision\nimport torchvision.transforms as transforms\nfrom sklearn.datasets import make_moons\nfrom torch.utils.data import TensorDataset, DataLoader\nfrom torchvision import datasets, transforms","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T22:49:31.572039Z","iopub.execute_input":"2025-03-05T22:49:31.572495Z","iopub.status.idle":"2025-03-05T22:49:31.576995Z","shell.execute_reply.started":"2025-03-05T22:49:31.572460Z","shell.execute_reply":"2025-03-05T22:49:31.576077Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"# Exemple le plus simple possible sur un dataset généré avec le plus petit modèle possible pour tester les gradients","metadata":{}},{"cell_type":"code","source":"# Modèle simple de classification\nclass SimpleNN(nn.Module):\n    def __init__(self, input_dim, hidden_dim, output_dim):\n        super(SimpleNN, self).__init__()\n        self.fc1 = nn.Linear(input_dim, hidden_dim)\n        self.fc2 = nn.Linear(hidden_dim, output_dim)\n    \n    def forward(self, x):\n        x = F.relu(self.fc1(x))\n        return self.fc2(x)\n\n# Fonction pour mettre à jour les paramètres du modèle\ndef set_model_params(model, theta):\n    with torch.no_grad():\n        for param, new_param in zip(model.parameters(), theta):\n            param.copy_(new_param)\n\n# Fonction de log-probabilité avec prior gaussien\ndef log_prob_func(model, data, target, prior_std=1.0):\n    logits = model(data)\n    log_likelihood = -F.cross_entropy(logits, target, reduction='sum')\n    prior = -0.5 * sum(torch.sum(p ** 2) for p in model.parameters()) / (prior_std ** 2)\n    return log_likelihood + prior\n\n# Fonction pour collecter les gradients\ndef compute_gradients(model, data, target):\n    log_prob = log_prob_func(model, data, target)\n    grads = torch.autograd.grad(log_prob, model.parameters(), create_graph=True)\n    return grads\n\n# Implémentation de Leapfrog\ndef leapfrog(theta, r, step_size, num_steps, model, data, target):\n    theta = [p.clone().detach().requires_grad_(True) for p in theta]\n    r = [ri.clone().detach() for ri in r]\n    \n    set_model_params(model, theta)\n    grad = compute_gradients(model, data, target)\n    \n    for i in range(len(r)):\n        r[i] = r[i] + 0.5 * step_size * grad[i]\n    \n    for _ in range(num_steps):\n        for i in range(len(theta)):\n            theta[i] = theta[i] + step_size * r[i]\n        \n        set_model_params(model, theta)\n        grad = compute_gradients(model, data, target)\n        \n        for i in range(len(r)):\n            r[i] = r[i] + step_size * grad[i]\n    \n    set_model_params(model, theta)\n    grad = compute_gradients(model, data, target)\n    \n    for i in range(len(r)):\n        r[i] = r[i] - 0.5 * step_size * grad[i]\n    \n    return theta, r\n\n# Fonction d'acceptation Metropolis-Hastings\ndef acceptance(theta, r, new_theta, new_r, model, data, target):\n    set_model_params(model, theta)\n    current_H = -log_prob_func(model, data, target) + 0.5 * sum(torch.sum(ri**2) for ri in r)\n    set_model_params(model, new_theta)\n    proposed_H = -log_prob_func(model, data, target) + 0.5 * sum(torch.sum(ri**2) for ri in new_r)\n    \n    accept_prob = torch.exp(current_H - proposed_H)\n    if torch.rand(1) < accept_prob:\n        return new_theta, 1\n    else:\n        return theta, 0","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Hyperparamètres\ninput_dim = 2\nhidden_dim = 16\noutput_dim = 2\nnum_samples = 100\nstep_size = 0.01\nnum_steps = 10\nbatch_size = 32\n\n# Chargement des données (Two Moons)\nX, y = make_moons(n_samples=1000, noise=0.1)\nX = torch.tensor(X, dtype=torch.float32)\ny = torch.tensor(y, dtype=torch.long)\ndataset = TensorDataset(X, y)\ndataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n\n# Initialisation du modèle\nmodel = SimpleNN(input_dim, hidden_dim, output_dim)\ntheta = [p.clone().detach() for p in model.parameters()]\n\n# Échantillonnage HMC\nsamples = []\nratio_acceptations = 0\nfor _ in range(num_samples):\n    r = [torch.randn_like(p) for p in theta]\n    data, target = next(iter(dataloader))\n    new_theta, new_r = leapfrog(theta, r, step_size, num_steps, model, data, target)\n    theta, acceptation = acceptance(theta, r, new_theta, new_r, model, data, target)\n    ratio_acceptations += acceptation\n    samples.append([p.clone().detach() for p in theta])\n\nprint(f\"Échantillonnage terminé avec {len(samples)} échantillons et un ratio de {ratio_acceptations/num_samples}.\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Charger le dataset de test\nX_test, y_test = make_moons(n_samples=300, noise=0.1)\nX_test = torch.tensor(X_test, dtype=torch.float32)\ny_test = torch.tensor(y_test, dtype=torch.long)\n\n# Initialiser les prédictions accumulées\npredictions = torch.zeros((len(samples), X_test.shape[0], output_dim))\n\n# Effectuer des prédictions avec chaque échantillon\nfor i, sample in enumerate(samples):\n    set_model_params(model, sample)  # Charger les paramètres échantillonnés\n    logits = model(X_test)  # Prédictions sur le test set\n    predictions[i] = F.softmax(logits, dim=1)  # Convertir en probabilités\n\n# Moyenne des prédictions (Bayesian Model Averaging)\navg_predictions = predictions.mean(dim=0)  # Moyenne sur tous les échantillons\n\n# Prédictions finales (classe avec probabilité max)\ny_pred = avg_predictions.argmax(dim=1)\n\n# Calcul de l'accuracy\naccuracy = (y_pred == y_test).float().mean().item()\nprint(f\"Accuracy (BMA): {accuracy:.4f}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"On obtient 90% d'accuracy","metadata":{}},{"cell_type":"markdown","source":"# Test sur MNIST avec des CNN (plus dur)","metadata":{}},{"cell_type":"code","source":"# Modèle de classification avec plus de couches\nclass DeepNN(nn.Module):\n    def __init__(self, input_dim, hidden_dim, output_dim):\n        super(DeepNN, self).__init__()\n        self.fc1 = nn.Linear(input_dim, hidden_dim)\n        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n        self.fc3 = nn.Linear(hidden_dim, hidden_dim)\n        self.fc4 = nn.Linear(hidden_dim, output_dim)\n    \n    def forward(self, x):\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = F.relu(self.fc3(x))\n        return self.fc4(x)\n\n\nclass CNN(nn.Module):\n    def __init__(self, output_dim=10):\n        super(CNN, self).__init__()\n        \n        # Définir les couches convolutives\n        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, stride=1, padding=1)  # Images MNIST (1 canal)\n        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\n        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1)\n        \n        # Couches fully-connected\n        self.fc1 = nn.Linear(128 * 3 * 3, 512)  # Taille après réduction par les convolutions (28x28 -> 3x3 après convolutions et pooling)\n        self.fc2 = nn.Linear(512, output_dim)  # 10 classes pour MNIST\n        \n        # Couches de pooling\n        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n    \n    def forward(self, x):\n        x = self.pool(F.relu(self.conv1(x)))  # Conv1 + ReLU + Pooling\n        x = self.pool(F.relu(self.conv2(x)))  # Conv2 + ReLU + Pooling\n        x = self.pool(F.relu(self.conv3(x)))  # Conv3 + ReLU + Pooling\n        x = x.view(-1, 128 * 3 * 3)  # Aplatir les données pour la couche fully-connected\n        x = F.relu(self.fc1(x))  # Fully connected 1\n        x = self.fc2(x)  # Fully connected 2 (sortie)\n        return x\n\n\n# Fonction pour mettre à jour les paramètres du modèle\ndef set_model_params(model, theta):\n    with torch.no_grad():\n        for param, new_param in zip(model.parameters(), theta):\n            param.copy_(new_param)\n\n# Fonction de log-probabilité avec prior gaussien\ndef log_prob_func(model, data, target, prior_std=1.0):\n    logits = model(data)\n    log_likelihood = -F.cross_entropy(logits, target, reduction='sum')\n    prior = -0.5 * sum(torch.sum(p ** 2) for p in model.parameters()) / (prior_std ** 2)\n    return log_likelihood + prior\n\n# Fonction pour collecter les gradients\ndef compute_gradients(model, data, target):\n    log_prob = log_prob_func(model, data, target)\n    grads = torch.autograd.grad(log_prob, model.parameters(), create_graph=True)\n    return grads\n\n# Implémentation de Leapfrog\ndef leapfrog(theta, r, step_size, num_steps, model, data, target):\n    theta = [p.clone().detach().requires_grad_(True) for p in theta]\n    r = [ri.clone().detach() for ri in r]\n    \n    set_model_params(model, theta)\n    grad = compute_gradients(model, data, target)\n    \n    for i in range(len(r)):\n        r[i] = r[i] + 0.5 * step_size * grad[i]\n    \n    for _ in range(num_steps):\n        for i in range(len(theta)):\n            theta[i] = theta[i] + step_size * r[i]\n        \n        set_model_params(model, theta)\n        grad = compute_gradients(model, data, target)\n        \n        for i in range(len(r)):\n            r[i] = r[i] + step_size * grad[i]\n    \n    set_model_params(model, theta)\n    grad = compute_gradients(model, data, target)\n    \n    for i in range(len(r)):\n        r[i] = r[i] - 0.5 * step_size * grad[i]\n    \n    return theta, r\n\n# Fonction d'acceptation Metropolis-Hastings\ndef acceptance(theta, r, new_theta, new_r, model, data, target):\n    set_model_params(model, theta)\n    current_H = -log_prob_func(model, data, target) + 0.5 * sum(torch.sum(ri**2) for ri in r)\n    set_model_params(model, new_theta)\n    proposed_H = -log_prob_func(model, data, target) + 0.5 * sum(torch.sum(ri**2) for ri in new_r)\n    \n    accept_prob = torch.exp(current_H - proposed_H)\n    if torch.rand(1).to(device) < accept_prob:\n        #print(accept_prob)\n        return new_theta, 1\n    else:\n        return theta, 0","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T22:50:22.144301Z","iopub.execute_input":"2025-03-05T22:50:22.144696Z","iopub.status.idle":"2025-03-05T22:50:22.159370Z","shell.execute_reply.started":"2025-03-05T22:50:22.144667Z","shell.execute_reply":"2025-03-05T22:50:22.158372Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"### Génération des samples","metadata":{}},{"cell_type":"code","source":"# Vérifier si un GPU est disponible\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Hyperparamètres\noutput_dim = 10  # MNIST a 10 classes\nnum_samples = 200\nstep_size = 0.001\nnum_steps = 10\nbatch_size = 64\n\n# Chargement des données MNIST\ntransform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])  # MNIST est en niveaux de gris\n\ntrainset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\ntrainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True)\n\n# Initialisation du modèle et envoi du modèle sur le GPU\nmodel = CNN(output_dim).to(device)  # Envoyer le modèle sur le GPU\ntheta = [p.clone().detach().to(device) for p in model.parameters()]  # Déplacer les paramètres du modèle sur le GPU\n\n# Échantillonnage HMC\nsamples = []\nratio_acceptations = 0\nfor _ in range(num_samples):\n    r = [torch.randn_like(p).to(device) for p in theta]  # Initialisation de r sur GPU\n    data, target = next(iter(trainloader))\n    data, target = data.to(device), target.to(device)  # Envoyer les données sur le GPU\n\n    new_theta, new_r = leapfrog(theta, r, step_size, num_steps, model, data, target)\n    theta, acceptation = acceptance(theta, r, new_theta, new_r, model, data, target)\n    ratio_acceptations += acceptation\n    samples.append([p.clone().detach().to(device) for p in theta])  # Stocker sur GPU\n\nprint(f\"Échantillonnage terminé avec {len(samples)} échantillons et un ratio de {ratio_acceptations/num_samples}.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T22:50:23.807303Z","iopub.execute_input":"2025-03-05T22:50:23.807619Z","iopub.status.idle":"2025-03-05T22:50:37.446464Z","shell.execute_reply.started":"2025-03-05T22:50:23.807598Z","shell.execute_reply":"2025-03-05T22:50:37.445539Z"}},"outputs":[{"name":"stdout","text":"Échantillonnage terminé avec 200 échantillons et un ratio de 0.985.\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"### Calcul de l'accuracy","metadata":{}},{"cell_type":"code","source":"\ntestset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\ntestloader = DataLoader(testset, batch_size=batch_size, shuffle=False)\n\n# Fonction d'inférence avec la méthode BMA\ndef bma_inference(model, samples, data):\n    # On suppose que `samples` est une liste des paramètres du modèle (après HMC)\n    # On va faire une prédiction pour chaque échantillon\n    preds = []\n    for sample in samples:\n        # Charger les paramètres dans le modèle\n        for param, s in zip(model.parameters(), sample):\n            param.data = s.data  # Mettre à jour les paramètres du modèle avec l'échantillon\n        \n        # Prédire avec le modèle actuel\n        output = model(data)\n        preds.append(F.softmax(output, dim=1))  # Utilisation de softmax pour obtenir les probabilités\n    \n    # Calcul de la moyenne des prédictions\n    avg_preds = torch.stack(preds).mean(0)  # Moyenne sur tous les échantillons (axis 0)\n    \n    # Retourner la classe avec la plus haute probabilité\n    return avg_preds.argmax(dim=1)\n\n# Calcul de l'accuracy sur le testset\nmodel.eval()  # Mettre le modèle en mode évaluation\ncorrect = 0\ntotal = 0\nwith torch.no_grad():\n    for data, target in testloader:\n        data, target = data.to(device), target.to(device)  # Envoyer les données sur le GPU\n        \n        # Obtenez les prédictions avec BMA\n        preds = bma_inference(model, samples, data)\n        \n        # Compter les prédictions correctes\n        correct += (preds == target).sum().item()\n        total += target.size(0)\n\naccuracy = correct / total\nprint(f\"Accuracy sur le testset : {accuracy * 100:.2f}%\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T22:53:15.426719Z","iopub.execute_input":"2025-03-05T22:53:15.427129Z","iopub.status.idle":"2025-03-05T22:53:33.532251Z","shell.execute_reply.started":"2025-03-05T22:53:15.427097Z","shell.execute_reply":"2025-03-05T22:53:33.531311Z"}},"outputs":[{"name":"stdout","text":"Accuracy sur le testset : 75.90%\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"### 75% d'accuracy enfinnnnnnnnnnnnnnnnn\n","metadata":{}}]}