{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What are BNNs posteriors really like???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chargement des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FashionMNIST_CSV(Dataset): # Dataset pytorch\n",
    "    def __init__(self, csv_path, transform=None):\n",
    "        self.data = pd.read_csv(csv_path).values  # Charger le CSV en NumPy\n",
    "        self.labels = self.data[:, 0]  # Première colonne = labels\n",
    "        self.images = self.data[:, 1:].reshape(-1, 1, 28, 28).astype(np.float32)  # Reshape en (N, 1, 28, 28)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return torch.tensor(image), torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "def get_fashion_mnist_loaders_from_csv(batch_size=128):\n",
    "    #train-set 60000 images, test-set 10000\n",
    "    train_dataset = FashionMNIST_CSV(\"data/fashion-mnist_train.csv\", transform=lambda x: (x / 255.0))  # Normalisation [0,1]\n",
    "    test_dataset = FashionMNIST_CSV(\"data/fashion-mnist_test.csv\", transform=lambda x: (x / 255.0))\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, test_loader\n",
    "\n",
    "\n",
    "train_loader, test_loader = get_fashion_mnist_loaders_from_csv(batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian Neural Network (ResNet quoi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1, use_bias=True, activation=nn.ReLU):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=use_bias)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.activation = activation()\n",
    "\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=use_bias)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=use_bias),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.activation(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        return self.activation(out)\n",
    "\n",
    "class ResNet20(nn.Module):\n",
    "    def __init__(self, num_classes=10, width=8, activation=nn.ReLU): # width 16 initialement\n",
    "        super(ResNet20, self).__init__()\n",
    "        self.num_blocks = 3  # ResNet-20 has 3 blocks per stage\n",
    "        self.in_channels = width\n",
    "        self.activation = activation()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(1, width, kernel_size=3, stride=1, padding=1, bias=True) #1 canal\n",
    "        self.bn1 = nn.BatchNorm2d(width)\n",
    "\n",
    "        self.stage1 = self._make_layer(width, self.num_blocks, stride=1)\n",
    "        self.stage2 = self._make_layer(width * 2, self.num_blocks, stride=2)\n",
    "        self.stage3 = self._make_layer(width * 4, self.num_blocks, stride=2)\n",
    "\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(width * 4, num_classes)\n",
    "\n",
    "    def _make_layer(self, out_channels, num_blocks, stride):\n",
    "        layers = []\n",
    "        layers.append(BasicBlock(self.in_channels, out_channels, stride))\n",
    "        self.in_channels = out_channels\n",
    "        for _ in range(1, num_blocks):\n",
    "            layers.append(BasicBlock(out_channels, out_channels))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.activation(self.bn1(self.conv1(x)))\n",
    "        out = self.stage1(out)\n",
    "        out = self.stage2(out)\n",
    "        out = self.stage3(out)\n",
    "        out = self.avg_pool(out)\n",
    "        out = torch.flatten(out, 1)\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fonctions utilitaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_weights(model, new_weights):\n",
    "    \"\"\"Remplace les poids d'un modèle PyTorch par ceux issus de HMC\"\"\"\n",
    "    state_dict = model.state_dict()  # Dictionnaire des paramètres du modèle\n",
    "    param_keys = [name_layer for name_layer, _ in list(model.named_parameters())]\n",
    "\n",
    "    #param_keys = list(state_dict.keys()) # liste des paramètres du modèle\n",
    "\n",
    "    #with torch.no_grad():\n",
    "    for i, key in enumerate(param_keys):\n",
    "        state_dict[key] = new_weights[i]\n",
    "    model.load_state_dict(state_dict) # charge les nouveaux paramètres\n",
    "\n",
    "def model_predictions(model, dataloader):\n",
    "    \"\"\"Fait des prédictions d'un modèle donné sur tout le dataset et stocke les probabilités\"\"\"\n",
    "    \n",
    "    probabilities = []\n",
    "    model.eval() \n",
    "    with torch.no_grad():  # Désactive le calcul des gradients pour la prédiction\n",
    "        for step, (image, label) in enumerate(dataloader):  # itère sur le dataset\n",
    "            image = image.device()  \n",
    "            logits = model(image)\n",
    "            probs = F.softmax(logits, dim=1)  \n",
    "            probabilities.append(probs)\n",
    "    return torch.cat(probabilities, dim=0)  # (dataset_size, num_classes)\n",
    "\n",
    "def BMA_predictions(probabilities):\n",
    "    \"\"\"Fait une prédiction moyenne Bayesian Model Average p(y|x, D) = 1/M * sum_i( p(y|x, wi))\n",
    "    Args:\n",
    "        probabilities: Tensor  (n_models, dataset_size, num_classes)\"\"\"\n",
    "    \n",
    "    n_models = probabilities.size(0)  # Nombre de modèles\n",
    "    # Moyenne des prédictions sur la première dimension (celles des modèles)\n",
    "    average_predictions = probabilities.mean(dim=0)  # (dataset_size, num_classes)\n",
    "    \n",
    "    # Prédiction finale : la classe qui a la probabilité la plus élevée\n",
    "    class_predict = average_predictions.argmax(dim=1)  # (dataset_size)\n",
    "    return class_predict\n",
    "\n",
    "    \n",
    "def calculate_accuracy(predictions, labels):\n",
    "    \"\"\"Calcule l'accuracy en comparant les prédictions aux labels.\"\"\"\n",
    "    correct_predictions = (predictions == labels).sum().item()  # Nombre de prédictions correctes\n",
    "    total_predictions = labels.size(0)  # Nombre total d'exemples\n",
    "    accuracy = correct_predictions / total_predictions  # Accuracy\n",
    "    return accuracy   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor = torch.tensor(np.array([[1, 2], [3, 4]]))\n",
    "tensor.size()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fonctions de densité"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "def posterior_log_density_func(model, data_loader, weight_decay, device):\n",
    "    \"\"\" \n",
    "    Approximation stochastique du posterior: log p(w|D) = log p(D|w) + log p(w) - log p(D)\n",
    "    p(D) est une constante, on ne la calcule pas\n",
    "    \"\"\"\n",
    "    # Échantillonne un mini-batch\n",
    "    data, target = next(iter(data_loader))  \n",
    "    data, target = data.to(device), target.to(device)\n",
    "\n",
    "    # Calcule la log-vraisemblance (log p(D | w))\n",
    "    output = model(data)\n",
    "    loss = F.cross_entropy(output, target, reduction=\"sum\")  # NLL\n",
    "    log_p_data = -loss  # On prend le négatif car HMC maximise log-likelihood\n",
    "\n",
    "    # Calcule le log-prior (log p(w))\n",
    "    log_p_w = -0.5 * sum(torch.sum(p**2) for p in model.parameters()) * weight_decay # + constante\n",
    "\n",
    "    # f(w) = log p(D | w) + log p(w)\n",
    "    f_w = log_p_data + log_p_w \n",
    "    return f_w\n",
    "\n",
    "\n",
    "def stochastic_grad_f(model, data_loader, weight_decay, device):\n",
    "    \"\"\"Approximation stochastique du gradient ∇f(w) avec un mini-batch\"\"\"\n",
    "    model.zero_grad()\n",
    "    \n",
    "    # Échantillonne un mini-batch\n",
    "    data, target = next(iter(data_loader))  \n",
    "    data, target = data.to(device), target.to(device)\n",
    "\n",
    "    # Calcule la log-vraisemblance (log p(D | w))\n",
    "    output = model(data)\n",
    "    loss = F.cross_entropy(output, target, reduction=\"sum\")  # NLL\n",
    "    log_p_data = -loss  # On prend le négatif car HMC maximise log-likelihood\n",
    "\n",
    "    # Calcule le log-prior (log p(w))\n",
    "    log_p_w = -0.5 * sum(torch.sum(p**2) for p in model.parameters()) * weight_decay\n",
    "\n",
    "    # f(w) = log p(D | w) + log p(w)\n",
    "    f_w = log_p_data + log_p_w  \n",
    "\n",
    "    # Gradient ∇f(w)\n",
    "    (-f_w).backward()  \n",
    "    gradients = [p.grad for p in model.parameters()]\n",
    "    \n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithme MHC minibatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sg_leapfrog(w, m, delta, n_leapfrog, model, data_loader, weight_decay, eta, device):\n",
    "    \"\"\"\n",
    "    Version Stochastic Gradient Leapfrog conforme à l'algorithme du papier.\n",
    "    \"\"\"\n",
    "    for _ in range(n_leapfrog):\n",
    "        # Étape 1 : Mise à jour du momentum à mi-chemin\n",
    "        grad_w = stochastic_grad_f(model, data_loader, weight_decay, device)\n",
    "        for i, p in enumerate(model.parameters()):\n",
    "            noise = torch.normal(mean=0, std=np.sqrt(2 * eta * delta), size=p.shape, device=device)\n",
    "            m[i] = m[i] + (delta / 2) * grad_w[i] + noise  # Ajout du bruit SG-HMC\n",
    "\n",
    "        # Étape 2 : Mise à jour des poids\n",
    "        with torch.no_grad():\n",
    "            for i, p in enumerate(model.parameters()):\n",
    "                p += delta * m[i]\n",
    "\n",
    "        # Étape 3 : Dernière mise à jour du momentum\n",
    "        grad_w = stochastic_grad_f(model, data_loader, weight_decay, device)\n",
    "        for i, p in enumerate(model.parameters()):\n",
    "            m[i] = m[i] + (delta / 2) * grad_w[i]  # Pas de bruit ici, car déjà ajouté avant\n",
    "    \n",
    "    return w, [-mi for mi in m]  # Inversion du momentum pour réversibilité\n",
    "\n",
    "\n",
    "def SG_HMC(trajectory_length, n_burnin, model, data_loader, delta, n_samples, weight_decay, eta, device):\n",
    "    \"\"\"\n",
    "    Stochastic Gradient Hamiltonian Monte Carlo (SG-HMC)\n",
    "    \n",
    "    Args:\n",
    "        trajectory_length : Longueur de trajectoire\n",
    "        n_burnin : Nombre d'itérations de burn-in\n",
    "        model : Réseau de neurones (torch.nn.Module)\n",
    "        data_loader : DataLoader pour mini-batchs\n",
    "        delta : Pas d'intégration pour Leapfrog\n",
    "        n_samples : Nombre d'échantillons à générer\n",
    "        weight_decay : Coefficient pour le log-prior gaussien\n",
    "        eta : Coefficient de bruit correctif pour SG-HMC\n",
    "        device : CPU ou GPU\n",
    "    \n",
    "    Returns:\n",
    "        Liste des échantillons de poids\n",
    "    \"\"\"\n",
    "    n_leapfrog = int(trajectory_length / delta)\n",
    "    model.to(device)\n",
    "    \n",
    "    # Initialisation des poids et des moments\n",
    "    w = [p for p in model.parameters()]\n",
    "    m = [torch.normal(mean=torch.zeros_like(p), std=torch.ones_like(p)) for p in model.parameters()]\n",
    "    \n",
    "    # Burn-in phase\n",
    "    for _ in tqdm(range(n_burnin), desc=\"Burn-in\"):\n",
    "        m = [torch.normal(mean=torch.zeros_like(p), std=torch.ones_like(p)) for p in model.parameters()]\n",
    "        w, m = sg_leapfrog(w, m, delta, n_leapfrog, model, data_loader, weight_decay, eta, device)\n",
    "    \n",
    "    # Échantillonnage\n",
    "    w_samples = []\n",
    "    for _ in tqdm(range(n_samples), desc=\"Sampling\"):\n",
    "        m = [torch.normal(mean=torch.zeros_like(p), std=torch.ones_like(p)) for p in model.parameters()]\n",
    "        w_proposed, m_proposed = sg_leapfrog(w, m, delta, n_leapfrog, model, data_loader, weight_decay, eta, device)\n",
    "        \n",
    "        # Metropolis-Hastings correction\n",
    "        model_copy = model.copy()\n",
    "        set_weights(model_copy, w_proposed)\n",
    "        log_acceptance_ratio = (\n",
    "        posterior_log_density_func(w_proposed) - posterior_log_density_func(w) \n",
    "        + 0.5 * (torch.norm(m) ** 2 - torch.norm(m_proposed) ** 2)    )\n",
    "\n",
    "        if torch.rand(1) < torch.exp(log_acceptance_ratio):\n",
    "            w = w_proposed\n",
    "        \n",
    "        w_samples.append([p.clone().detach() for p in w])\n",
    "    \n",
    "    return w_samples  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet = ResNet20(num_classes=10)  # Création du modèle\n",
    "hmc_weights = [torch.randn_like(p) for p in resnet.parameters()]  # Simulation de poids HMC\n",
    "set_weights(resnet, hmc_weights)  # Injection des nouveaux poids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Script global"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the log posterior density function\n",
      "Computing the gradient of the log posterior density function\n",
      "Running SG-HMC\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Burn-in:   0%|          | 0/10 [00:34<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[187], line 28\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m### HMC et BMA predictions ###\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRunning SG-HMC\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 28\u001b[0m w_samples \u001b[38;5;241m=\u001b[39m \u001b[43mSG_HMC\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrajectory_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrajectory_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_burnin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_burnin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdelta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     36\u001b[0m \u001b[43m    \u001b[49m\u001b[43meta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     37\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\n\u001b[0;32m     38\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# Faire des prédictions avec les échantillons de poids\u001b[39;00m\n\u001b[0;32m     41\u001b[0m probabilities \u001b[38;5;241m=\u001b[39m []\n",
      "Cell \u001b[1;32mIn[185], line 53\u001b[0m, in \u001b[0;36mSG_HMC\u001b[1;34m(trajectory_length, n_burnin, model, data_loader, delta, n_samples, weight_decay, eta, device)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(n_burnin), desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBurn-in\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     52\u001b[0m     m \u001b[38;5;241m=\u001b[39m [torch\u001b[38;5;241m.\u001b[39mnormal(mean\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mzeros_like(p), std\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mones_like(p)) \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mparameters()]\n\u001b[1;32m---> 53\u001b[0m     w, m \u001b[38;5;241m=\u001b[39m \u001b[43msg_leapfrog\u001b[49m\u001b[43m(\u001b[49m\u001b[43mw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdelta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_leapfrog\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# Échantillonnage\u001b[39;00m\n\u001b[0;32m     56\u001b[0m w_samples \u001b[38;5;241m=\u001b[39m []\n",
      "Cell \u001b[1;32mIn[185], line 7\u001b[0m, in \u001b[0;36msg_leapfrog\u001b[1;34m(w, m, delta, n_leapfrog, model, data_loader, weight_decay, eta, device)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03mVersion Stochastic Gradient Leapfrog conforme à l'algorithme du papier.\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_leapfrog):\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;66;03m# Étape 1 : Mise à jour du momentum à mi-chemin\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m     grad_w \u001b[38;5;241m=\u001b[39m \u001b[43mstochastic_grad_f\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, p \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(model\u001b[38;5;241m.\u001b[39mparameters()):\n\u001b[0;32m      9\u001b[0m         noise \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnormal(mean\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, std\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39msqrt(\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m eta \u001b[38;5;241m*\u001b[39m delta), size\u001b[38;5;241m=\u001b[39mp\u001b[38;5;241m.\u001b[39mshape, device\u001b[38;5;241m=\u001b[39mdevice)\n",
      "Cell \u001b[1;32mIn[184], line 43\u001b[0m, in \u001b[0;36mstochastic_grad_f\u001b[1;34m(model, data_loader, weight_decay, device)\u001b[0m\n\u001b[0;32m     40\u001b[0m f_w \u001b[38;5;241m=\u001b[39m log_p_data \u001b[38;5;241m+\u001b[39m log_p_w  \n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# Gradient ∇f(w)\u001b[39;00m\n\u001b[1;32m---> 43\u001b[0m \u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mf_w\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \n\u001b[0;32m     44\u001b[0m gradients \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mgrad \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mparameters()]\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m gradients\n",
      "File \u001b[1;32mc:\\Users\\Franc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    580\u001b[0m     )\n\u001b[1;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Franc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Franc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\autograd\\graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "### Hyper-paramètres ###\n",
    "\n",
    "# Paramètres du prior gaussien :\n",
    "prior_variance = 1/5\n",
    "std = np.sqrt(prior_variance)\n",
    "weight_decay = 1/(std**2) # definition\n",
    "\n",
    "# Paramètres HMC :\n",
    "trajectory_length = (np.pi*std)/2 # formule du papier\n",
    "n_burnin = 10 # 50 dans le papier\n",
    "delta = 1e-5 # 1e-5, 5e-5, 1e-4 dans le papier\n",
    "n_samples = 240\n",
    "eta = 1e-5\n",
    "\n",
    "# Choix du modèle et des fonctions\n",
    "model = ResNet20(num_classes=10)\n",
    "f = posterior_log_density_func(model, train_loader, weight_decay, device)\n",
    "grad_f = stochastic_grad_f(model, train_loader, weight_decay, device)\n",
    "\n",
    "# Initialise les poids du modèle suivant le prior\n",
    "w_init = [torch.normal(mean=0, std=std, size=p.shape) for p in model.parameters()] \n",
    "set_weights(model, w_init)\n",
    "\n",
    "### HMC et BMA predictions ###\n",
    "w_samples = SG_HMC(\n",
    "    trajectory_length=trajectory_length,\n",
    "    n_burnin=n_burnin,\n",
    "    model=model,\n",
    "    data_loader=train_loader,\n",
    "    delta=delta,\n",
    "    n_samples=n_samples,\n",
    "    weight_decay=weight_decay,\n",
    "    eta=eta, \n",
    "    device=device\n",
    ")\n",
    "\n",
    "# Faire des prédictions avec les échantillons de poids\n",
    "probabilities = []\n",
    "for w_sample in w_samples:\n",
    "    set_weights(model, w_sample)\n",
    "    probabilities.append(model_predictions(model, test_loader))\n",
    "\n",
    "probabilities = torch.stack(probabilities)\n",
    "class_predictions = BMA_predictions(probabilities)\n",
    "accuracy = calculate_accuracy(class_predictions, test_loader.dataset.targets)\n",
    "print(f'Accuracy: {accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
