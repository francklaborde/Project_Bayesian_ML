{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU"},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nimport torchvision\nimport torchvision.transforms as transforms\nfrom sklearn.datasets import make_moons\nfrom torch.utils.data import TensorDataset, DataLoader\nfrom torchvision import datasets, transforms\n\nfrom tqdm import tqdm","metadata":{"execution":{"iopub.status.busy":"2025-03-07T17:28:05.895528Z","iopub.execute_input":"2025-03-07T17:28:05.895799Z","iopub.status.idle":"2025-03-07T17:28:12.203203Z","shell.execute_reply.started":"2025-03-07T17:28:05.895772Z","shell.execute_reply":"2025-03-07T17:28:12.202458Z"},"trusted":true,"id":"xXbOA9RM91_o"},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"# Exemple le plus simple possible sur un dataset généré avec le plus petit modèle possible pour tester les gradients","metadata":{"id":"_guWf3MA91_r"}},{"cell_type":"code","source":"# Modèle simple de classification\nclass SimpleNN(nn.Module):\n    def __init__(self, input_dim, hidden_dim, output_dim):\n        super(SimpleNN, self).__init__()\n        self.fc1 = nn.Linear(input_dim, hidden_dim)\n        self.fc2 = nn.Linear(hidden_dim, output_dim)\n\n    def forward(self, x):\n        x = F.relu(self.fc1(x))\n        return self.fc2(x)\n\n# Fonction pour mettre à jour les paramètres du modèle\ndef set_model_params(model, theta):\n    with torch.no_grad():\n        for param, new_param in zip(model.parameters(), theta):\n            param.copy_(new_param)\n\n# Fonction de log-probabilité avec prior gaussien\ndef log_prob_func(model, data, target, prior_std=1.0):\n    logits = model(data)\n    log_likelihood = -F.cross_entropy(logits, target, reduction='sum')\n    prior = -0.5 * sum(torch.sum(p ** 2) for p in model.parameters()) / (prior_std ** 2)\n    return log_likelihood + prior\n\n# Fonction pour collecter les gradients\ndef compute_gradients(model, data, target):\n    log_prob = log_prob_func(model, data, target)\n    grads = torch.autograd.grad(log_prob, model.parameters(), create_graph=True)\n    return grads\n\n# Implémentation de Leapfrog\ndef leapfrog(theta, r, step_size, num_steps, model, data, target):\n    theta = [p.clone().detach().requires_grad_(True) for p in theta]\n    r = [ri.clone().detach() for ri in r]\n\n    set_model_params(model, theta)\n    grad = compute_gradients(model, data, target)\n\n    for i in range(len(r)):\n        r[i] = r[i] + 0.5 * step_size * grad[i]\n\n    for _ in range(num_steps):\n        for i in range(len(theta)):\n            theta[i] = theta[i] + step_size * r[i]\n\n        set_model_params(model, theta)\n        grad = compute_gradients(model, data, target)\n\n        for i in range(len(r)):\n            r[i] = r[i] + step_size * grad[i]\n\n    set_model_params(model, theta)\n    grad = compute_gradients(model, data, target)\n\n    for i in range(len(r)):\n        r[i] = r[i] - 0.5 * step_size * grad[i]\n\n    return theta, r\n\n# Fonction d'acceptation Metropolis-Hastings\ndef acceptance(theta, r, new_theta, new_r, model, data, target):\n    set_model_params(model, theta)\n    current_H = -log_prob_func(model, data, target) + 0.5 * sum(torch.sum(ri**2) for ri in r)\n    set_model_params(model, new_theta)\n    proposed_H = -log_prob_func(model, data, target) + 0.5 * sum(torch.sum(ri**2) for ri in new_r)\n\n    accept_prob = torch.exp(current_H - proposed_H)\n    if torch.rand(1) < accept_prob:\n        return new_theta, 1\n    else:\n        return theta, 0","metadata":{"trusted":true,"id":"g4BWY3uf91_r","execution":{"iopub.status.busy":"2025-03-07T17:28:12.203881Z","iopub.execute_input":"2025-03-07T17:28:12.204326Z","iopub.status.idle":"2025-03-07T17:28:12.214896Z","shell.execute_reply.started":"2025-03-07T17:28:12.204292Z","shell.execute_reply":"2025-03-07T17:28:12.214018Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# Hyperparamètres\ninput_dim = 2\nhidden_dim = 16\noutput_dim = 2\nnum_samples = 100\nstep_size = 0.01\nnum_steps = 10\nbatch_size = 32\n\n# Chargement des données (Two Moons)\nX, y = make_moons(n_samples=1000, noise=0.1)\nX = torch.tensor(X, dtype=torch.float32)\ny = torch.tensor(y, dtype=torch.long)\ndataset = TensorDataset(X, y)\ndataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n\n# Initialisation du modèle\nmodel = SimpleNN(input_dim, hidden_dim, output_dim)\ntheta = [p.clone().detach() for p in model.parameters()]\n\n# Échantillonnage HMC\nsamples = []\nratio_acceptations = 0\nfor _ in range(num_samples):\n    r = [torch.randn_like(p) for p in theta]\n    data, target = next(iter(dataloader))\n    new_theta, new_r = leapfrog(theta, r, step_size, num_steps, model, data, target)\n    theta, acceptation = acceptance(theta, r, new_theta, new_r, model, data, target)\n    ratio_acceptations += acceptation\n    samples.append([p.clone().detach() for p in theta])\n\nprint(f\"Échantillonnage terminé avec {len(samples)} échantillons et un ratio de {ratio_acceptations/num_samples}.\")\n","metadata":{"trusted":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"G_rO2_Un91_s","outputId":"b184cdd1-7e62-476a-f00b-8977dade1cf6","execution":{"iopub.status.busy":"2025-03-07T17:28:12.215717Z","iopub.execute_input":"2025-03-07T17:28:12.215933Z","iopub.status.idle":"2025-03-07T17:28:13.899665Z","shell.execute_reply.started":"2025-03-07T17:28:12.215915Z","shell.execute_reply":"2025-03-07T17:28:13.898750Z"}},"outputs":[{"name":"stdout","text":"Échantillonnage terminé avec 100 échantillons et un ratio de 0.99.\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# Charger le dataset de test\nX_test, y_test = make_moons(n_samples=300, noise=0.1)\nX_test = torch.tensor(X_test, dtype=torch.float32)\ny_test = torch.tensor(y_test, dtype=torch.long)\n\n# Initialiser les prédictions accumulées\npredictions = torch.zeros((len(samples), X_test.shape[0], output_dim))\n\n# Effectuer des prédictions avec chaque échantillon\nfor i, sample in enumerate(samples):\n    set_model_params(model, sample)  # Charger les paramètres échantillonnés\n    logits = model(X_test)  # Prédictions sur le test set\n    predictions[i] = F.softmax(logits, dim=1)  # Convertir en probabilités\n\n# Moyenne des prédictions (Bayesian Model Averaging)\navg_predictions = predictions.mean(dim=0)  # Moyenne sur tous les échantillons\n\n# Prédictions finales (classe avec probabilité max)\ny_pred = avg_predictions.argmax(dim=1)\n\n# Calcul de l'accuracy\naccuracy = (y_pred == y_test).float().mean().item()\nprint(f\"Accuracy (BMA): {accuracy:.4f}\")\n","metadata":{"trusted":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"YZGyOrBu91_t","outputId":"4c897735-d188-45a9-d4ce-8572b9c5b5ec","execution":{"iopub.status.busy":"2025-03-07T17:28:13.901392Z","iopub.execute_input":"2025-03-07T17:28:13.901643Z","iopub.status.idle":"2025-03-07T17:28:13.942427Z","shell.execute_reply.started":"2025-03-07T17:28:13.901621Z","shell.execute_reply":"2025-03-07T17:28:13.941591Z"}},"outputs":[{"name":"stdout","text":"Accuracy (BMA): 0.8800\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"On obtient 90% d'accuracy","metadata":{"id":"IJ86Yjlj91_t"}},{"cell_type":"markdown","source":"# Test sur MNIST avec des CNN (plus dur)","metadata":{"id":"t5JNdE_J91_u"}},{"cell_type":"code","source":"# Modèle de classification avec plus de couches\nclass DeepNN(nn.Module):\n    def __init__(self, input_dim, hidden_dim, output_dim):\n        super(DeepNN, self).__init__()\n        self.fc1 = nn.Linear(input_dim, hidden_dim)\n        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n        self.fc3 = nn.Linear(hidden_dim, hidden_dim)\n        self.fc4 = nn.Linear(hidden_dim, output_dim)\n\n    def forward(self, x):\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = F.relu(self.fc3(x))\n        return self.fc4(x)\n\n\nclass CNN(nn.Module):\n    def __init__(self, output_dim=10):\n        super(CNN, self).__init__()\n\n        # Définir les couches convolutives\n        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, stride=1, padding=1)  # Images MNIST (1 canal)\n        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\n        #self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1)\n\n        # Couches fully-connected\n        self.fc1 = nn.Linear(64 * 7 * 7, 512)  # Taille après réduction par les convolutions (28x28 -> 3x3 après convolutions et pooling)\n        self.fc2 = nn.Linear(512, output_dim)  # 10 classes pour MNIST\n\n        # Couches de pooling\n        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n\n    def forward(self, x):\n        x = self.pool(F.relu(self.conv1(x)))  # Conv1 + ReLU + Pooling\n        x = self.pool(F.relu(self.conv2(x)))  # Conv2 + ReLU + Pooling\n        #x = self.pool(F.relu(self.conv3(x)))  # Conv3 + ReLU + Pooling\n        x = x.view(-1, 64 * 7 * 7)  # Aplatir les données pour la couche fully-connected\n        x = F.relu(self.fc1(x))  # Fully connected 1\n        x = self.fc2(x)  # Fully connected 2 (sortie)\n        return x\n\n\n# Fonction pour mettre à jour les paramètres du modèle\ndef set_model_params(model, theta):\n    with torch.no_grad():\n        for param, new_param in zip(model.parameters(), theta):\n            param.copy_(new_param)\n\n# Fonction de log-probabilité avec prior gaussien\ndef log_prob_func(model, data, target, prior_std=1.0):\n    logits = model(data)\n    log_likelihood = -F.cross_entropy(logits, target, reduction='sum')\n    prior = -0.5 * sum(torch.sum(p ** 2) for p in model.parameters()) / (prior_std ** 2)\n    return log_likelihood + prior\n\n# Fonction pour collecter les gradients\ndef compute_gradients(model, data, target):\n    log_prob = log_prob_func(model, data, target)\n    grads = torch.autograd.grad(log_prob, model.parameters(), create_graph=True)\n    return grads\n\n# Implémentation de Leapfrog\ndef leapfrog(theta, r, step_size, num_steps, model, data, target):\n    theta = [p.clone().detach().requires_grad_(True) for p in theta]\n    r = [ri.clone().detach() for ri in r]\n\n    set_model_params(model, theta)\n    grad = compute_gradients(model, data, target)\n\n    for i in range(len(r)):\n        r[i] = r[i] + 0.5 * step_size * grad[i]\n\n    for _ in range(num_steps):\n        for i in range(len(theta)):\n            theta[i] = theta[i] + step_size * r[i]\n\n        set_model_params(model, theta)\n        grad = compute_gradients(model, data, target)\n\n        for i in range(len(r)):\n            r[i] = r[i] + step_size * grad[i]\n\n    set_model_params(model, theta)\n    grad = compute_gradients(model, data, target)\n\n    for i in range(len(r)):\n        r[i] = r[i] - 0.5 * step_size * grad[i]\n\n    return theta, r\n\n# Fonction d'acceptation Metropolis-Hastings\ndef acceptance(theta, r, new_theta, new_r, model, data, target, device):\n    set_model_params(model, theta)\n    current_H = -log_prob_func(model, data, target) + 0.5 * sum(torch.sum(ri**2) for ri in r)\n\n    set_model_params(model, new_theta)\n    proposed_H = -log_prob_func(model, data, target) + 0.5 * sum(torch.sum(ri**2) for ri in new_r)\n\n    accept_prob = torch.exp(current_H - proposed_H)\n\n    if torch.rand(1).to(device) < accept_prob:\n        #print(accept_prob)\n        return new_theta, 1\n    else:\n        return theta, 0\n\ndef HMC_update(trajectory_length, N_burnin, model, trainloader, delta, num_sample, device, N_leapfrog=None):\n    theta = [p.clone().detach().to(device) for p in model.parameters()]  # Déplacer les paramètres du modèle sur le GPU\n    if N_leapfrog is None:\n      N_leapfrog = int(trajectory_length // delta)\n\n    if N_leapfrog > 240:\n        N_leapfrog = 240 # With N_leapfrog too high, the memory is saturated\n    print(f\"Number of leapfrog steps : {N_leapfrog}\")\n\n    # Burn in phase\n    for _ in tqdm(range(N_burnin), desc=\"Burn in phase\"):\n      data, target = next(iter(trainloader))\n      data, target = data.to(device), target.to(device)  # Envoyer les données sur le GPU\n      r = [torch.randn_like(p).to(device) for p in theta]\n      new_theta, _ = leapfrog(theta, r, delta, N_leapfrog, model, data, target)\n      theta = new_theta\n\n      # Reduce memory cost\n      del r, new_theta\n      torch.cuda.empty_cache()\n    # Échantillonnage HMC\n    samples = []\n    ratio_acceptations = 0\n    for _ in tqdm(range(num_sample), desc=\"HMC sampling\"):\n        r = [torch.randn_like(p).to(device) for p in theta]  # Initialisation de r sur GPU\n        data, target = next(iter(trainloader))\n        data, target = data.to(device), target.to(device)  # Envoyer les données sur le GPU\n\n        new_theta, new_r = leapfrog(theta, r, delta, N_leapfrog, model, data, target)\n        theta, acceptation = acceptance(theta, r, new_theta, new_r, model, data, target, device)\n        ratio_acceptations += acceptation\n\n        # Delete the cache and variable to reduce the memory cost\n        del r, new_theta, new_r\n        torch.cuda.empty_cache()\n\n        samples.append([p.clone().detach().cpu() for p in theta])\n    print(f\"Échantillonnage terminé avec {len(samples)} échantillons et un ratio de {ratio_acceptations/num_sample}.\")\n    return samples, ratio_acceptations/num_sample","metadata":{"execution":{"iopub.status.busy":"2025-03-07T17:28:13.943864Z","iopub.execute_input":"2025-03-07T17:28:13.944140Z","iopub.status.idle":"2025-03-07T17:28:13.961493Z","shell.execute_reply.started":"2025-03-07T17:28:13.944120Z","shell.execute_reply":"2025-03-07T17:28:13.960752Z"},"trusted":true,"id":"mMeJL5-a91_u"},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"### Génération des samples","metadata":{"id":"IzDBh8o691_u"}},{"cell_type":"code","source":"# Vérifier si un GPU est disponible\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Hyperparamètres\noutput_dim = 10  # MNIST a 10 classes\nnum_samples = 200\nstep_size = 0.001\nnum_steps = 10\nbatch_size = 64\n\n# Chargement des données MNIST\ntransform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])  # MNIST est en niveaux de gris\n\ntrainset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\ntrainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True)\n\n# Initialisation du modèle et envoi du modèle sur le GPU\nmodel = CNN(output_dim).to(device) # Envoyer le modèle sur le GPU\ntheta = [p.clone().detach().to(device) for p in model.parameters()]  # Déplacer les paramètres du modèle sur le GPU\n\n# Parameters for the HMC\ntau_hat = torch.pi*torch.std(trainset.data.float() / 255)/2 # Trajectory length estimated in the paper\ndelta = 1e-3\nN_burnin = 10\nN_leapfrog = 100\n\n# samples, ratio = HMC_update(tau_hat, N_burnin, model, trainloader, delta, num_samples, device, N_leapfrog)\n\n","metadata":{"execution":{"iopub.status.busy":"2025-03-07T17:28:13.962223Z","iopub.execute_input":"2025-03-07T17:28:13.962479Z","iopub.status.idle":"2025-03-07T17:28:14.491115Z","shell.execute_reply.started":"2025-03-07T17:28:13.962459Z","shell.execute_reply":"2025-03-07T17:28:14.490453Z"},"trusted":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"pXL-KS1t91_u","outputId":"3f93bea0-45ef-4b3a-c48a-2ecb4cbb903c"},"outputs":[],"execution_count":6},{"cell_type":"code","source":"\n# # Échantillonnage HMC\n# samples = []\n# ratio_acceptations = 0\n# for _ in range(num_samples):\n#     r = [torch.randn_like(p).to(device) for p in theta]  # Initialisation de r sur GPU\n#     data, target = next(iter(trainloader))\n#     data, target = data.to(device), target.to(device)  # Envoyer les données sur le GPU\n\n#     new_theta, new_r = leapfrog(theta, r, step_size, num_steps, model, data, target)\n#     theta, acceptation = acceptance(theta, r, new_theta, new_r, model, data, target, device)\n#     ratio_acceptations += acceptation\n#     samples.append([p.clone().detach().to(device) for p in theta])  # Stocker sur GPU\n\n# print(f\"Échantillonnage terminé avec {len(samples)} échantillons et un ratio de {ratio_acceptations/num_samples}.\")","metadata":{"id":"0vy7cv0RZueP","trusted":true,"execution":{"iopub.status.busy":"2025-03-07T17:28:14.491907Z","iopub.execute_input":"2025-03-07T17:28:14.492215Z","iopub.status.idle":"2025-03-07T17:28:14.495671Z","shell.execute_reply.started":"2025-03-07T17:28:14.492186Z","shell.execute_reply":"2025-03-07T17:28:14.494836Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"","metadata":{"id":"UkNJOj5LTJ3v"}},{"cell_type":"markdown","source":"### Calcul de l'accuracy","metadata":{"id":"Y5SLI59i91_v"}},{"cell_type":"code","source":"testset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\ntestloader = DataLoader(testset, batch_size=batch_size, shuffle=False, drop_last=True) # drop_last to stack the outputs and target later\n\n# Fonction d'inférence avec la méthode BMA\ndef bma_inference(model, samples, data, device):\n    # On suppose que `samples` est une liste des paramètres du modèle (après HMC)\n    # On va faire une prédiction pour chaque échantillon\n    preds = []\n    for sample in samples:\n        # Charger les paramètres dans le modèle\n        for param, s in zip(model.parameters(), sample):\n            param.data = s.data  # Mettre à jour les paramètres du modèle avec l'échantillon\n\n        model.to(device)  # Mettre le modèle sur le GPU\n        # Prédire avec le modèle actuel\n        output = model(data)\n        preds.append(F.softmax(output, dim=1))  # Utilisation de softmax pour obtenir les probabilités\n\n    # Calcul de la moyenne des prédictions\n    avg_preds = torch.stack(preds).mean(0)  # Moyenne sur tous les échantillons (axis 0)\n\n    # Retourner la classe avec la plus haute probabilité\n    return avg_preds, avg_preds.argmax(dim=1)\n\n# # Calcul de l'accuracy sur le testset\n# model.eval()  # Mettre le modèle en mode évaluation\n# correct = 0\n# total = 0\n# with torch.no_grad():\n#     for data, target in tqdm(testloader, desc=\"Predicting the test set\"):\n#         data, target = data.to(device), target.to(device)  # Envoyer les données sur le GPU\n\n#         # Obtenez les prédictions avec BMA\n#         _, preds = bma_inference(model, samples, data, device)\n\n#         # Compter les prédictions correctes\n#         correct += (preds == target).sum().item()\n#         total += target.size(0)\n\n# accuracy = correct / total\n# print(f\"Accuracy sur le testset : {accuracy * 100:.2f}%\")","metadata":{"execution":{"iopub.status.busy":"2025-03-07T17:28:14.496298Z","iopub.execute_input":"2025-03-07T17:28:14.496555Z","iopub.status.idle":"2025-03-07T17:28:14.528473Z","shell.execute_reply.started":"2025-03-07T17:28:14.496536Z","shell.execute_reply":"2025-03-07T17:28:14.527668Z"},"trusted":true,"id":"gAD7dFdO91_v"},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"### 75% d'accuracy enfinnnnnnnnnnnnnnnnn\n","metadata":{"id":"MeRkITNX91_v"}},{"cell_type":"markdown","source":"## Tests on the parameters","metadata":{"id":"YzjPNuDuZ8yW"}},{"cell_type":"markdown","source":"Function to compute all the metrics :\n- The Expected Calibration Error (ECE)\n- The accuracy\n- The confidence","metadata":{"id":"3n7R6VFu7-xO"}},{"cell_type":"code","source":"def compute_calibration_curve(outputs, labels, num_bins=20):\n    \"\"\"\n    Compute the calibration curve and Expected Calibration Error (ECE).\n\n    Parameters:\n        outputs (torch.Tensor): Model outputs (probabilities), shape (N, num_classes).\n        labels (torch.Tensor): Ground-truth labels, shape (N,).\n        num_bins (int): Number of bins for calibration.\n\n    Returns:\n        dict: Contains \"confidence\", \"accuracy\", \"proportions\", and \"ece\".\n    \"\"\"\n    # Flatten outputs and labels\n    labels = labels.view(-1)  # Ensure labels are 1D\n    outputs = outputs.view(labels.size(0), -1)  # Ensure correct shape\n\n    # Get confidences (max probabilities) and predicted classes\n    confidences, predictions = torch.max(outputs, dim=1)\n    accuracies = (predictions == labels).float()  # Boolean to float (1=correct, 0=incorrect)\n\n    # Sort confidences to determine bin edges\n    num_inputs = confidences.shape[0]\n    bins = torch.sort(confidences)[0][::(num_inputs + num_bins - 1) // num_bins]\n    if num_inputs % ((num_inputs + num_bins - 1) // num_bins) != 1:\n        bins = torch.cat((bins, confidences.max().unsqueeze(0)))\n\n    # Initialize bin statistics\n    bin_confidences, bin_accuracies, bin_proportions = [], [], []\n    ece = 0.0  # Expected Calibration Error\n\n    # Compute calibration metrics for each bin\n    for bin_lower, bin_upper in zip(bins[:-1], bins[1:]):\n        in_bin = (confidences > bin_lower) & (confidences <= bin_upper)\n        prop_in_bin = in_bin.float().mean().item()\n\n        if prop_in_bin > 0:\n            accuracy_in_bin = accuracies[in_bin].mean().item()\n            avg_confidence_in_bin = confidences[in_bin].mean().item()\n            ece += abs(avg_confidence_in_bin - accuracy_in_bin) * prop_in_bin\n\n            bin_confidences.append(avg_confidence_in_bin)\n            bin_accuracies.append(accuracy_in_bin)\n            bin_proportions.append(prop_in_bin)\n\n    # Convert lists to tensors\n    return {\n        \"confidence\": torch.tensor(bin_confidences),\n        \"accuracy\": torch.tensor(bin_accuracies),\n        \"proportions\": torch.tensor(bin_proportions),\n        \"ece\": ece\n    }\n","metadata":{"id":"PmQK5lB27-TH","trusted":true,"execution":{"iopub.status.busy":"2025-03-07T17:28:14.529161Z","iopub.execute_input":"2025-03-07T17:28:14.529355Z","iopub.status.idle":"2025-03-07T17:28:14.535865Z","shell.execute_reply.started":"2025-03-07T17:28:14.529339Z","shell.execute_reply":"2025-03-07T17:28:14.535008Z"}},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"Now that we ensure the model works, we will reproduce the experience on the trajectory length and hmc chain\n","metadata":{"id":"VBY3RbjRaBil"}},{"cell_type":"code","source":"def hmc_test(trajectory_length, N_burnin, model, trainloader, delta, num_sample, testloader, samples, device, N_leapfrog=None, num_classes=10):\n  model.to(device)\n  # HMC update\n  samples, ratio = HMC_update(trajectory_length, N_burnin, model, trainloader, delta, num_sample, device, N_leapfrog)\n\n  outputs = []\n  targets = []\n  model.eval()  # Mettre le modèle en mode évaluation\n  with torch.no_grad():\n      for data, target in tqdm(testloader, desc=\"Predicting the test set\"):\n          data, target = data.to(device), target.to(device)  # Envoyer les données sur le GPU\n\n          # Obtenez les prédictions avec BMA\n          preds, avg_pred = bma_inference(model, samples, data, device)\n\n          outputs.append(preds)\n          targets.append(target)\n  outputs = torch.stack(outputs).view(-1, num_classes)\n  targets = torch.stack(targets).view(-1)\n\n  results = compute_calibration_curve(outputs, targets)\n\n  print(\"\\nAccuracy : \", results[\"accuracy\"].mean().item())\n  print(\"Confidence : \", results[\"confidence\"].mean().item())\n  print(\"ECE : \", results[\"ece\"])\n\n  log_likelihood = -F.cross_entropy(outputs, targets, reduction='sum')\n  print(\"log likelihood : \", log_likelihood.item())\n  return results\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YjpEJdMDaBAd","outputId":"75725065-d45c-456c-ada2-271f67d3744e","trusted":true,"execution":{"iopub.status.busy":"2025-03-07T17:28:14.536545Z","iopub.execute_input":"2025-03-07T17:28:14.536764Z","iopub.status.idle":"2025-03-07T17:28:14.557321Z","shell.execute_reply.started":"2025-03-07T17:28:14.536745Z","shell.execute_reply":"2025-03-07T17:28:14.556528Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"tau_hat = torch.pi*torch.std(trainset.data.float() / 255)/2 # Trajectory length estimated in the paper\ndelta = 1e-3\nN_burnin = 10\nN_leapfrog = 100\n\nprop_tau_test = [0.25, 0.5, 1, 1.25, 1.5]\nfor prop in prop_tau_test:\n  tau_test = prop*tau_hat\n  print(f\"-------------- Trajectory length : {tau_test} -------------------\")\n  results = hmc_test(tau_test, N_burnin, model, trainloader, delta, num_samples, testloader, samples, device)\n","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":391},"id":"PVJIiJC8-76Q","outputId":"77fb07ac-b052-4263-8b06-7726566a2a00","trusted":true,"execution":{"iopub.status.busy":"2025-03-07T17:28:14.558155Z","iopub.execute_input":"2025-03-07T17:28:14.558421Z","iopub.status.idle":"2025-03-07T17:50:55.373527Z","shell.execute_reply.started":"2025-03-07T17:28:14.558393Z","shell.execute_reply":"2025-03-07T17:50:55.372680Z"}},"outputs":[{"name":"stdout","text":"-------------- Trajectory length : 0.12099365890026093 -------------------\nNumber of leapfrog steps : 120\n","output_type":"stream"},{"name":"stderr","text":"Burn in phase: 100%|██████████| 10/10 [00:06<00:00,  1.56it/s]\nHMC sampling: 100%|██████████| 200/200 [01:43<00:00,  1.94it/s]\n","output_type":"stream"},{"name":"stdout","text":"Échantillonnage terminé avec 200 échantillons et un ratio de 0.455.\n","output_type":"stream"},{"name":"stderr","text":"Predicting the test set: 100%|██████████| 156/156 [01:11<00:00,  2.19it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nAccuracy :  0.9184038043022156\nConfidence :  0.4635261595249176\nECE :  0.4550315805553581\nlog likelihood :  -19554.802734375\n-------------- Trajectory length : 0.24198731780052185 -------------------\nNumber of leapfrog steps : 240\n","output_type":"stream"},{"name":"stderr","text":"Burn in phase: 100%|██████████| 10/10 [00:11<00:00,  1.10s/it]\nHMC sampling: 100%|██████████| 200/200 [03:31<00:00,  1.06s/it]\n","output_type":"stream"},{"name":"stdout","text":"Échantillonnage terminé avec 200 échantillons et un ratio de 0.51.\n","output_type":"stream"},{"name":"stderr","text":"Predicting the test set: 100%|██████████| 156/156 [01:11<00:00,  2.19it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nAccuracy :  0.13660551607608795\nConfidence :  0.1252305954694748\nECE :  0.032164164619650604\nlog likelihood :  -22974.376953125\n-------------- Trajectory length : 0.4839746356010437 -------------------\nNumber of leapfrog steps : 240\n","output_type":"stream"},{"name":"stderr","text":"Burn in phase: 100%|██████████| 10/10 [00:11<00:00,  1.16s/it]\nHMC sampling: 100%|██████████| 200/200 [03:33<00:00,  1.07s/it]\n","output_type":"stream"},{"name":"stdout","text":"Échantillonnage terminé avec 200 échantillons et un ratio de 0.555.\n","output_type":"stream"},{"name":"stderr","text":"Predicting the test set: 100%|██████████| 156/156 [01:11<00:00,  2.19it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nAccuracy :  0.1387372761964798\nConfidence :  0.12710696458816528\nECE :  0.03743731230318903\nlog likelihood :  -22973.671875\n-------------- Trajectory length : 0.6049683094024658 -------------------\nNumber of leapfrog steps : 240\n","output_type":"stream"},{"name":"stderr","text":"Burn in phase: 100%|██████████| 10/10 [00:11<00:00,  1.12s/it]\nHMC sampling: 100%|██████████| 200/200 [03:32<00:00,  1.06s/it]\n","output_type":"stream"},{"name":"stdout","text":"Échantillonnage terminé avec 200 échantillons et un ratio de 0.6.\n","output_type":"stream"},{"name":"stderr","text":"Predicting the test set: 100%|██████████| 156/156 [01:11<00:00,  2.19it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nAccuracy :  0.12203727662563324\nConfidence :  0.1267998218536377\nECE :  0.03300230029812401\nlog likelihood :  -22976.17578125\n-------------- Trajectory length : 0.7259619235992432 -------------------\nNumber of leapfrog steps : 240\n","output_type":"stream"},{"name":"stderr","text":"Burn in phase: 100%|██████████| 10/10 [00:11<00:00,  1.14s/it]\nHMC sampling: 100%|██████████| 200/200 [03:31<00:00,  1.06s/it]\n","output_type":"stream"},{"name":"stdout","text":"Échantillonnage terminé avec 200 échantillons et un ratio de 0.555.\n","output_type":"stream"},{"name":"stderr","text":"Predicting the test set: 100%|██████████| 156/156 [01:11<00:00,  2.19it/s]","output_type":"stream"},{"name":"stdout","text":"\nAccuracy :  0.12677502632141113\nConfidence :  0.12262680381536484\nECE :  0.03595462811980893\nlog likelihood :  -22980.140625\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"","metadata":{"id":"WjSVjjl6Gnau","trusted":true},"outputs":[],"execution_count":null}]}